[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "1A Wednesday/Friday: 8am-8:50am (Kaufman Hall 101)  1B Wednesday/Friday: 9am-9:50am (Kaufman Hall 101)"
  },
  {
    "objectID": "schedule.html#discussion",
    "href": "schedule.html#discussion",
    "title": "Schedule",
    "section": "",
    "text": "1A Wednesday/Friday: 8am-8:50am (Kaufman Hall 101)  1B Wednesday/Friday: 9am-9:50am (Kaufman Hall 101)"
  },
  {
    "objectID": "schedule.html#office-hours",
    "href": "schedule.html#office-hours",
    "title": "Schedule",
    "section": "Office Hours",
    "text": "Office Hours\nPlease stop by office hours and introduce yourself! Office hours are useful for one-on-one help on assignments, discussion of topics discussed in the reading, or broader questions about statistics and data science\nJose Toledo Luna: Monday: 10:00am-11:00am, Wednesday: 5:00pm -6:00pm on Zoom\nIf you can not attend the scheduled office hours I am available for a one-on-one appoitment, follow the directions on setting up an appointment"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Material from this course can be downloaded from this Box Folder\nOffice hours can be accessed through Zoom"
  },
  {
    "objectID": "notes/06-logical-statements.html#logical-statements",
    "href": "notes/06-logical-statements.html#logical-statements",
    "title": "Logical Statements with Applications",
    "section": "Logical Statements",
    "text": "Logical Statements\nThere are only two logical values, TRUE and FALSE. In R, we can abbreviate TRUE with T and FALSE with F. They can be interpreted as any option corresponding to a binary choice. For example, yes/no, do/don’t, satisfied/not satisfied or even 1/0.\nA basic way to define a logical statement is using a relational operator to compare two expressions. For example, we may ask ourselves “is x less than a certain number ?” or using a real world example from the mtcars dataset “how many cars have more than 18 miles per gallon?”\n\nRelational operators\nThe table below summarizes some of the relational operators available in R:\n\n\n\nOperator\nInterpretation\nBasic Example\nResult\n\n\n\n\n==\nEqual to\n5 == 5\nTRUE\n\n\n!=\nNot equal to\n4 != 5\nTRUE\n\n\n&gt;\nGreater than\n4 &gt; 5\nFALSE\n\n\n&lt;\nLess than\n4 &lt; 5\nTRUE\n\n\n&lt;=\nLess than or equal to\n4 &lt;= 5\nTRUE\n\n\n&gt;=\nGreater than or equal to\n4 &gt;= 5\nFALSE\n\n\n\nFrom the table above we consider single numbers as our two expression to compare, but we can extend this idea to vectors, data.frames, matrices of various data types. When applying relational operators to vectors it is important to know they are being compared element-wise.\nWe first start off by comparing a vector with a single number\n\nc(1,3,5,7,9) &lt; 5\n\n#&gt; [1]  TRUE  TRUE FALSE FALSE FALSE\n\n\nInterpretation: Is 1 less than 5? is 3 less than 5? is 5 less than 5? is 7 less than 5? is 9 less than 5?\nThe output from the above example is a logical vector\n\nclass(c(1,3,5,7,9) &lt; 5)\n\n#&gt; [1] \"logical\"\n\n\nwith TRUE/FALSE if the given condition was satisfied or not. What if we were given the question “How many values of x are smaller than some number?”\n\nsum( c(1,3,5,7,9) &lt; 5 )\n\n#&gt; [1] 2\n\n\nwe can then apply the sum() function to count how many TRUE were in our logical vector. This will be very useful when we have very large vectors and we can’t count how many TRUE were in our vector manually.\nBelow are some examples applying relational operators to compare two vectors of the same length\n\nc(1,2,3,4) &lt; c(5,4,3,2)\n\n#&gt; [1]  TRUE  TRUE FALSE FALSE\n\n\nInterpretation: Is 1 less than 5? is 2 less than 4? is 3 less than 3? is 4 less than 2?\n\nc(1,2,3,4) &lt;= c(5,4,3,2)\n\n#&gt; [1]  TRUE  TRUE  TRUE FALSE\n\n\nInterpretation: Is 1 less than or equal to 5? is 2 less than or equal to 4? is 3 less than or equal to 3? is 4 less than or equal to 2?\nAnother topic to consider is comparing two strings. While this can be a more advance topic we only consider the simplest scenario in which we compare case-sensitive strings.\n\nstring1 &lt;- 'Hello'\nstring2 &lt;- 'hello'\n\nwhile the above strings contain the same characters in the same order, if we compare them directly they will be considered different\n\nstring1 == string2\n\n#&gt; [1] FALSE\n\n\nInterpretation: are string1 and string2 identical?\nHowever, if were are interested in seeing if they contain the same characters regardless of the case sensitivity, we can use tolower() function as follows\n\ntolower(string1)\n\n#&gt; [1] \"hello\"\n\ntolower(string2)\n\n#&gt; [1] \"hello\"\n\n\ntolower() will convert any upper-case character in a vector into lower-case character.\n\ntolower(string1) == tolower(string2)\n\n#&gt; [1] TRUE\n\n\nSince all the characters are now lower-case, and both strings contain the same characters in the same order then they are now identical.\nFor more advanced examples in comparing strings check out the following blog post (Optional)\n\n\nLogical operators\nIn practice, we often need to use multiple conditions to make certain decisions. For example, you have a personal rule that if there is no homework AND you don’t have class, then you will go out with your friends. Now, explore what happens to this rule when OR is used instead of AND, also what happens when negation (NOT ) is added to one or both clauses.\nThe table below summarizes some of these logical operators\n\n\n\n\n\n\n\n\n\nOperator\nInterpretation\nBasic Example\nResult\n\n\n\n\n!\nNOT  If the condition is true, logical NOT operator returns as false\n! (5 == 5)\nFALSE\n\n\n&\nAND (element-wise)  Returns true when both conditions are true\nTRUE & TRUE\nTRUE & FALSE\nFALSE & TRUE\nFALSE & FALSE\nTRUE\nFALSE\nFALSE\nFALSE\n\n\n&&\nAND (single comparison)  Same as above but for single comparison\n(same as & above)\n(same as & above)\n\n\n|\nOR (element-wise)  Returns true when at-least one of conditions is true\nTRUE |TRUE\nTRUE | FALSE\nFALSE | TRUE\nFALSE | FALSE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\n||\nOR  (single comparison)  Same as above but for single comparison\n(same as | above)\n(same as | above)\n\n\n\nThe difference between element-wise and single comparison can be seen in the examples below\n\nc(TRUE,TRUE,FALSE,FALSE) | c(TRUE,FALSE,TRUE,FALSE)\n\n#&gt; [1]  TRUE  TRUE  TRUE FALSE\n\n\nInterpretation: TRUE or FALSE, TRUE or FALSE, FALSE or TRUE, FALSE or FALSE\nElement-wise will return a vector of logical values, one for each pair of logicals combined. Whereas, single comparison only compares the first two elements of the logical vectors and will return a single logical value\n\nage &lt;- 20\nage == 18 || age &lt;= 21\n\n#&gt; [1] TRUE\n\n\nInterpretation: Is age 18 OR less than or equal to 21 ?\n\nage &gt; 10 && age &lt; 30\n\n#&gt; [1] TRUE\n\n\nInterpretation: Is age greater than AND less than 30?\nConsider a more complicated example of holding office in the United States. The president must be a natural-born citizen of the United States, be at least 35 years old, and have been a resident of the United States for 14 years\n\ncandidate_age &lt;- 40\ncandidate_birth &lt;- 'United States'\ncandidate_residance_years &lt;- 10\n\nWe have a candidate who is 40 years old, was born in the United States but for some reason they have only been a resident of the United States for 10 years. Clearly, this candidate is not eligible to become our next president. We demonstrate this using logical operators\n\ncandidate_age &gt;= 35\n\n#&gt; [1] TRUE\n\n\nInterpretation: Is the candidate at least 35 years old?\n\ncandidate_birth == 'United States'\n\n#&gt; [1] TRUE\n\n\nInterpretation: Is the candidate born in United States?\n\ncandidate_residance_years &gt;= 14\n\n#&gt; [1] FALSE\n\n\nInterpretation: Has the candidate been a resident for at least 14 years?\nPutting all of the above together,\n\n(candidate_age &gt;= 35) && (candidate_birth == 'United States') && (candidate_residance_years &gt;= 14)\n\n#&gt; [1] FALSE\n\n\nInterpretation: TRUE AND TRUE AND FALSE\nSince one of the conditions fails the entire statement will be false."
  },
  {
    "objectID": "notes/06-logical-statements.html#subsetting",
    "href": "notes/06-logical-statements.html#subsetting",
    "title": "Logical Statements with Applications",
    "section": "Subsetting",
    "text": "Subsetting\n\nVectors\nNow that we have an idea of how to construct logical statements, we can apply them to subset our data based on a given condition\nConsider the following vector dat with 18 values\n\ndat &lt;- c(11, 13, 18, 3, 2, 24, 10, 8, 5, \n         13, 3, 23, 7, 25, 17, 20, 11, 17)\n\nWe will subset dat based on the following conditions:\n1. How many values are bigger than 10?\n\ndat &gt; 10 \n\n#&gt;  [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n#&gt; [13] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nsum(dat &gt; 10 )\n\n#&gt; [1] 11\n\n\nwhile knowing how many values are bigger than 10 is useful, we may only want to keep those values and not the ones that are smaller than 10.\n2. Keep the values that are bigger than 10?\nIf given a vector, the way to subset it based on a condition is as follows:  vector[ condtion ]. Our condition is all the values that are bigger than 10, that is dat &gt; 10\n\ndat[ dat &gt; 10 ]\n\n#&gt;  [1] 11 13 18 24 13 23 25 17 20 11 17\n\n\n3. How many values are exactly 11 ?\nOur condition is dat == 11,this should only return two TRUE, and after using the sum() function to count them we obtain\n\nsum(dat == 11)\n\n#&gt; [1] 2\n\n\nIf we wanted to extract these values from dat we would run\n\ndat[ dat == 11 ]\n\n#&gt; [1] 11 11\n\n\nNext we use the birth dataset for the following examples\n4. How many females were in this dataset?\n\nbirth_dat &lt;- read.csv(file = \"/Users/toledo60/Desktop/Projects/personal-website/data/births.csv\")\n\nFirst we extract the values from the Gender column and store them in a variable called gender_vec\n\ngender_vec &lt;- birth_dat$Gender\n\n\nunique(gender_vec)\n\n#&gt; [1] \"Male\"   \"Female\"\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRecall strings are case-sensitive, so you must spell ‘Female’ exactly as it appears above\n\n\nThen we subset this vector to only include females\n\nfemales_vec &lt;- gender_vec[gender_vec == 'Female']\n\n\nunique(females_vec)\n\n#&gt; [1] \"Female\"\n\n\nNow our vector only contains females, we can use length() to count how many females were in this dataset\n\nlength(females_vec)\n\n#&gt; [1] 961\n\n\nAn easier approach would be to simply create the variable gender_vec and count how many females are in that vector\n\nsum(gender_vec == 'Female')\n\n#&gt; [1] 961\n\n\n\n\nData Frames\nConsidering example 4 in the vectors section of subsetting, we are extracting solely the values from a specific column based on a given condition. However, in some scenarios we may want to preserve all other information (columns) from our dataset after subsetting our data.\nData frames have the following structure data[rows,columns]. The first argument inside the brackets will specify the rows and the second argument will specify the columns. We can apply all of the subsetting techniques we covered in the vectors within the rows, columns, or both rows and columns data[condition for rows, condition for columns]\nFor example, if we wanted to subset the births dataset to only include females\n\nis_female &lt;- birth_dat$Gender == 'Female'\n\n\nbirth_dat[is_female, ]\n\nInterpretation: Subset the rows to only include females, keep all the other columns\n\n\n#&gt;     X Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits\n#&gt; 5   5 Female     No    119      8   30   19    12    12       2     12\n#&gt; 9   9 Female     No    126      7   31   31    12    12       2      8\n#&gt; 10 10 Female     No    131      8   29   28     9     9       3      9\n#&gt;      Marital Racemom Racedad Hispmom Hispdad Gained     Habit MomPriorCond\n#&gt; 5  Unmarried   Black Unknown NotHisp Unknown     20 NonSmoker         None\n#&gt; 9    Married   White   White Mexican Mexican     30 NonSmoker         None\n#&gt; 10   Married   White   White Mexican Mexican     33 NonSmoker         None\n#&gt;    BirthDef DelivComp BirthComp\n#&gt; 5      None      None      None\n#&gt; 9      None      None      None\n#&gt; 10     None      None      None\n\n\nYou will notice that we only applied a condition to the rows argument and not the columns argument. In the case where one of the arguments is left blank, then no condition will be applied to the respective argument.\nFor practice, consider the following examples\n1. Create a new data frame containing the columns: Gender, weight, and Habit\nWe can use colnames()\n\ncolnames(birth_dat)\n\n#&gt;  [1] \"X\"            \"Gender\"       \"Premie\"       \"weight\"       \"Apgar1\"      \n#&gt;  [6] \"Fage\"         \"Mage\"         \"Feduc\"        \"Meduc\"        \"TotPreg\"     \n#&gt; [11] \"Visits\"       \"Marital\"      \"Racemom\"      \"Racedad\"      \"Hispmom\"     \n#&gt; [16] \"Hispdad\"      \"Gained\"       \"Habit\"        \"MomPriorCond\" \"BirthDef\"    \n#&gt; [21] \"DelivComp\"    \"BirthComp\"\n\n\nto make sure we have the correct spelling of the appropriate columns we want to keep.\n\nbirth2 &lt;- birth_dat[ , c('Gender','weight','Habit')]\n\nInterpretation: Keep all the rows, but only keep the columns: Gender, weight, and Habit\n\nhead(birth2,3)\n\n#&gt;   Gender weight     Habit\n#&gt; 1   Male    116 NonSmoker\n#&gt; 2   Male    126    Smoker\n#&gt; 3   Male    161 NonSmoker\n\n\nWe created a character vector with the names of the columns we wanted to keep and used it as the condition in the columns argument.\n2. Split birth_dat into two parts: One for which the individual was a smoker and another for which they were not a smoker\nThe variable Habit contains information on whether or not the individual was a smoker.\n\nunique(birth_dat$Habit)\n\n#&gt; [1] \"NonSmoker\" \"Smoker\"    \"\"\n\n\nFirst we create a logical vector to determine if the individual was a smoker\n\nis_smoker &lt;- birth_dat$Habit == 'Smoker'\n\n\nis_smoker[1:5]\n\n#&gt; [1] FALSE  TRUE FALSE FALSE FALSE\n\n\nInterpretation: Return TRUE if Habit is smoker, otherwise FALSE\nWe use the negation logical operator to obtain all the non-smokers from our logical vector is_smoker without having to create a new variable\n\n!is_smoker[1:5]\n\n#&gt; [1]  TRUE FALSE  TRUE  TRUE  TRUE\n\n\nTo subset our data into keeping only the smokers we input our logical vector is_smoker into the rows argument\n\nsmokers &lt;- birth_dat[is_smoker, ]\n\nInterpretation: Only keep the rows in which the individual is a smoker\n\nhead(smokers,3)\n\n#&gt;     X Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits\n#&gt; 2   2   Male     No    126      8   30   18    12    12       1     14\n#&gt; 16 16 Female    Yes     78      8   35   26    14    15       2      9\n#&gt; 19 19   Male     No    121      9   25   24    10    10       4     11\n#&gt;      Marital Racemom Racedad Hispmom Hispdad Gained  Habit MomPriorCond\n#&gt; 2  Unmarried   White Unknown NotHisp Unknown     50 Smoker At Least One\n#&gt; 16   Married   White   White NotHisp NotHisp     25 Smoker         None\n#&gt; 19 Unmarried   White   White NotHisp NotHisp     50 Smoker         None\n#&gt;    BirthDef    DelivComp BirthComp\n#&gt; 2      None         None      None\n#&gt; 16     None At Least One      None\n#&gt; 19     None         None      None\n\n\nTo subset our data into keeping only the non-smokers we input our logical vector !is_smoker into the rows argument\n\nnot_smokers &lt;- birth_dat[!is_smoker, ]\n\nInterpretation: Only keep the rows in which the individual is NOT a smoker\n\nhead(not_smokers,3)\n\n#&gt;   X Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits Marital\n#&gt; 1 1   Male     No    116      9   28   34     6     3       2     10 Married\n#&gt; 3 3   Male     No    161      8   28   29    12    12       3     14 Married\n#&gt; 4 4   Male     No    133      9   26   23     8     9       3     10 Married\n#&gt;   Racemom Racedad   Hispmom   Hispdad Gained     Habit MomPriorCond BirthDef\n#&gt; 1   White   White   Mexican   Mexican     30 NonSmoker         None     None\n#&gt; 3   White   White OtherHisp OtherHisp     65 NonSmoker         None     None\n#&gt; 4   White   White   Mexican   Mexican      8 NonSmoker         None     None\n#&gt;      DelivComp BirthComp\n#&gt; 1         None      None\n#&gt; 3 At Least One      None\n#&gt; 4 At Least One      None\n\n\n3. What is the average weight of babies with at least one birth defect?\nThe variable BirthDef determines if the baby had no birth defects or had at least one defect\n\nunique(birth_dat$BirthDef)\n\n#&gt; [1] \"None\"         \"At Least One\"\n\n\nCreate a logical vector to determine if the baby had at least one defect\n\nhas_defect &lt;- (birth_dat$BirthDef == 'At Least One')\n\n\n\n\n\n\n\nNote\n\n\n\nWe must spell “At Least One” with correct upper/lower cases including spaces\n\n\n\nhas_defect[1:5]\n\n#&gt; [1] FALSE FALSE FALSE FALSE FALSE\n\n\nSubset our data to include rows with babies with at least one defect, then select only the weight column. Lastly compute the mean.\n\nmean( birth_dat[has_defect,'weight'] )\n\n#&gt; [1] 115.8\n\n\nInterpretation: Average weight of babies with at least one birth defect"
  },
  {
    "objectID": "notes/06-logical-statements.html#missing-data",
    "href": "notes/06-logical-statements.html#missing-data",
    "title": "Logical Statements with Applications",
    "section": "Missing Data",
    "text": "Missing Data\nMissing data (or missing values) appear when no value is available in one or more variables of an observation. A common example can look something like this\n\n\n\n\n\nStudentID\nMajor\nGPA\n\n\n\n\n12345\nmath\n3.8\n\n\n23456\nNA\n3.2\n\n\n23405\nbiology\nNA\n\n\n\n\n\nwhere we do not know the major of the second student and we also do not know the major from the third student (denoted by NA)\nIdentifying the rows and columns where missing values occur is necessary before addressing the issue of missingness. Although it is easy to observe in the example mentioned above, in most cases, dealing with larger datasets requires a more programmatic approach\n\nVectors\nIn R, NA stands for “Not Available” and is used to represent missing values in a dataset. NA can be used for any data type in R, such as numeric character, or logical.\nThe type of NA is a logical value\n\ntypeof(NA)\n\n#&gt; [1] \"logical\"\n\n\nand can be coerced into any other data type. For example, consider the following numeric vector\n\ntypeof(c(1,2,3))\n\n#&gt; [1] \"double\"\n\n\nbut now with a missing value as the third element, it will preserve the original data type\n\ntypeof(c(1,2,NA,4))\n\n#&gt; [1] \"double\"\n\n\nor even a character vector\n\ntypeof(c(\"a\",\"b\",NA,\"c\"))\n\n#&gt; [1] \"character\"\n\n\nIn the following, we will show several examples how to find missing values. The most common approach is to use the function is.na()\n\nis.na(c(1,2,NA,4,NA))\n\n#&gt; [1] FALSE FALSE  TRUE FALSE  TRUE\n\n\nInterpretation: For each element does this element contain NA\nwhich will return a logical vector of the same length as the input vector, TRUE in the position which NA is located in. We can use the function which() in order to find out the actual position of TRUE\n\nwhich( is.na(c(1,2,NA,4,NA)) )\n\n#&gt; [1] 3 5\n\n\nInterpretation: Which position(s) are the logical values TRUE located\nThe output will then be an integer vector denoting the positions in which there were missing values. Applying the concepts learned in Subsetting, we can exclude any values which are missing. For example,\n\nx &lt;- c(1,2,NA,4,NA)\nis.na(x)\n\n#&gt; [1] FALSE FALSE  TRUE FALSE  TRUE\n\n\n\n!is.na(x)\n\n#&gt; [1]  TRUE  TRUE FALSE  TRUE FALSE\n\n\nInterpretation: For each element does this element NOT contain NA\n\nx[!is.na(x)]\n\n#&gt; [1] 1 2 4\n\n\nInterpretation: Only keep the elements which DO NOT contain NA\nIf we only want to find out if there any NA values, we can utilize the function anyNA()\n\nanyNA(c(1,2,NA,4,NA))\n\n#&gt; [1] TRUE\n\n\nThe above command will output TRUE if there are any NA in the vector and FALSE if there is not a single missing value\n\nanyNA(c(1,2,3))\n\n#&gt; [1] FALSE\n\n\nIn conclusion, a common approach to check for missing data in R, we can use is.na() or anyNA(). If we want to know the position of the missing values, we should use is.na(). However, if we are only concerned with whether there are any missing values or not, and not their position, then we can use anyNA()\n\n\nData Frames\nNow, working with data frames we would like to verify if there are any missing observations throughout the entire dataset\n\nstudent_dat &lt;- data.frame(\n  'StudentID' = c('12345', '23456', '23405'),\n  'Major' = c(\"math\",NA,\"biology\"),\n  'GPA' = c(3.8,3.2,NA))\n\n\n\n#&gt;   StudentID   Major GPA\n#&gt; 1     12345    math 3.8\n#&gt; 2     23456    &lt;NA&gt; 3.2\n#&gt; 3     23405 biology  NA\n\n\nWhen the function is.na() is applied to a data frame, the output will be a matrix containing logical values. The logical values in the matrix will depend on whether there were any missing values or not in the data frame\n\nis.na(student_dat)\n\n#&gt;      StudentID Major   GPA\n#&gt; [1,]     FALSE FALSE FALSE\n#&gt; [2,]     FALSE  TRUE FALSE\n#&gt; [3,]     FALSE FALSE  TRUE\n\n\nIf we wanted to find out the position(s) of the missing values for each column we will utilize the apply(). The basic syntax for apply() is\napply(X, MARGIN, FUN)\n\nx: an array or matrix\nMARGIN: take a value or range between 1 and 2 to define where to apply the function\nMARGIN=1: the manipulation is performed on rows\nMARGIN=2: the manipulation is performed on columns\nMARGIN=c(1,2): the manipulation is performed on rows and columns\nFUN: tells which function to apply, according to the specified MARGIN\n\n\napply(X = is.na(student_dat), MARGIN = 2, FUN = which)\n\n#&gt; $StudentID\n#&gt; integer(0)\n#&gt; \n#&gt; $Major\n#&gt; [1] 2\n#&gt; \n#&gt; $GPA\n#&gt; [1] 3\n\n\nInterpretation: From each column MARGIN =2, which values (FUN = which) from student_dat are missing is.na(student_dat)\nThe output of using apply(...,MARGIN =2) will be a list containing the row(s) in which missing values were found from each column.\nIn our case there were no rows with missing data in the first column, the second row contained a missing value from the column Major and the third row contained a missing value from the column GPA"
  },
  {
    "objectID": "notes/10-CLT.html",
    "href": "notes/10-CLT.html",
    "title": "Introduction to Central Limit Theorem",
    "section": "",
    "text": "library(ggplot2)\nlibrary(patchwork)\nlibrary(palmerpenguins)\ntheme_set(theme_bw())\ntheme_replace(panel.grid.minor = element_blank(),\n              panel.grid.major = element_blank())"
  },
  {
    "objectID": "notes/10-CLT.html#sampling-distributions",
    "href": "notes/10-CLT.html#sampling-distributions",
    "title": "Introduction to Central Limit Theorem",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\nSample mean\nThe sampling distribution of the sample mean is the theoretical distribution of means that would result from taking all possible samples of size \\(n\\) from the population.\nSuppose we are sampling from a population which comes from a standard Normal distribution such that observations in our sample are iid (independent, identically distributed). Each sample of size \\(n\\) will then consists of realizations \\(\\{x_1,\\dots,x_n\\}\\), such that each \\(x_i\\) will be a realization of the random variable \\(X_i \\sim N(0,1)\\)\nTo obtain the sampling distribution for the sample mean,\nFor the total number of samples do the following:\n\nGenerate a random sample of size \\(n\\) from the population\nCompute the sample mean and store the result in a variable, say sample_means\nPlot each of the sample_means using a histogram\n\n\nn_samples &lt;- 10000\nsample_size &lt;- 100 # each sample will be of size 100\nsample_means &lt;- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  rnorm(n = sample_size, mean = 0, sd=1) # generate a new sample\n  sample_means[i] = mean(sample_i) # obtain mean for each sample\n}\n\n\n\nShow Code\n\n\nsampling_dist &lt;- ggplot(data.frame(sample_means),aes(sample_means))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution',\n       x = 'Sample means',y = '')\n\n\n\n\n\n\n\nThe plot above is the distribution of sample means after taking 10,000 samples with size \\(n=100\\) from the population. The overall distribution appears to be normally distributed (bell shaped curve).\nThe Central Limit Theorem (CLT) states that if you have a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) and take sufficiently large random samples from the population with replacement, then the distribution of the sample means (sampling distribution) will be approximately normally distributed with mean \\(\\mu\\) and standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\). Note, the standard deviation of the sampling distribution of the sample mean (or any other statistic) is referred to as the standard error.\nRecall, the population is distributed as a standard normal distribution, i.e \\(N(\\mu=0,\\sigma=1)\\)\n\nmean(sample_means)\n\n#&gt; [1] -0.001400346\n\n\nwhich is very close to the population mean of \\(\\mu =0\\)\nwith standard error\n\nsd(sample_means)\n\n#&gt; [1] 0.1003452\n\n\nwhich is approximately \\(\\frac{\\sigma}{\\sqrt{n}} = \\frac{1}{10}\\)\nBelow is a demonstration of how increasing the sample size affects the distribution of the sample mean and seeing CLT in action\n\n\nShow Code\n\n\ngenerate_sampling_distribution &lt;- function(n_samples, mean = 0, sd = 1,\n                                           color = 'black', fill = 'steelblue',\n                                           alpha = 0.3, bins=30) {\n  \n  sample_means &lt;- numeric(n_samples)\n  \n  for(i in 1:n_samples){\n    sample_i =  rnorm(n = n_samples, mean = mean, sd=sd) # generate a new sample\n    sample_means[i] = mean(sample_i) # obtain mean for each sample\n  }\n  \n  sample_mean &lt;- round(mean(sample_means),3)\n  standard_error &lt;- round(sd(sample_means),3)\n  \n  sampling_dist &lt;- ggplot(data.frame(sample_means,n_samples),aes(sample_means))+\n    geom_histogram(fill = fill,alpha = alpha,\n                   color = color,bins=bins)+\n    labs(title = 'Sampling distribution',\n         x = 'Sample means',y = '',\n         subtitle = paste0('mean=',sample_mean,\n                           ', standard error=',standard_error))\n  return(sampling_dist+facet_grid(~n_samples))\n}\n\n\nplots &lt;- generate_sampling_distribution(100)+\n  generate_sampling_distribution(1000)+\n  generate_sampling_distribution(5000)+\n  generate_sampling_distribution(10000)+\n  plot_layout(nrow = 2, ncol = 2)\n\n\n\n\n\n\n\nTo interactively visualize how the sampling distribution of the sample mean builds up one sample at a time and see when the Central Limit Theorem start to kick in, I highly recommend the art of stats sampling distributions and Central Limit Theorem web applications.\n\n\nApplication: Sample Mean\nNow we will apply the the concepts of generating a sampling distribution for the sample mean using a real world dataset. In particular, we will use the penguins dataset from the palmerpenguins package.\nUsing the penguins dataset we will generate a sampling distribution for the sample mean of the penguins’ body mass (grams)\nFor simplicity, we will remove any missing observations\n\nbody_mass_g &lt;- penguins$body_mass_g[!is.na(penguins$body_mass_g)]\n\nRecall, to obtain the sampling distribution for the sample mean, do the following process\nFor the total number of samples do the following:\n\nGenerate a random sample of size \\(n\\) from the population, penguins$body_mass_g\nCompute the sample mean of the penguins body mass and store the result in a variable, say body_mass_xbar\nPlot each of the body_mass_xbar using a histogram\n\n\nn_samples &lt;- 10000\nsample_size &lt;- 50\n\nbody_mass_xbar &lt;- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  sample(body_mass_g, size = sample_size) # generate a new sample\n  body_mass_xbar[i] = mean(sample_i) # obtain mean for each sample\n}\n\n\n\nShow Code\n\n\nsampling_dist &lt;- ggplot(data.frame(body_mass_xbar),\n                        aes(body_mass_xbar))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution of Sample Mean',\n       subtitle = 'Penguins body mass (grams)',\n       x = 'Sample means',y = '')\n\n\n\n\n\n\n\nThe mean for the sampling distribution is\n\nmean(body_mass_xbar)\n\n#&gt; [1] 4201.996\n\n\nwhich is approximately the population mean for the penguins body mass (without missing observations)\n\nmean(body_mass_g)\n\n#&gt; [1] 4201.754\n\n\nthe standard error for the sampling distribution is\n\nsd(body_mass_xbar)\n\n#&gt; [1] 105.3449\n\n\n\n\nSample proportions\nThe idea behind generating a sampling distribution for sample proportions is very similar to the procedure described in Sample means section.\nTo obtain the sampling distribution for the sample proportions;\nFor the total number of samples do the following:\n\nGenerate a random sample of size \\(n\\) from the population\nCompute the sample proportion and store the result in a variable, say sample_proportions\nPlot each of the sample_proportions using a histogram\n\nHere the sample proportion is the fraction of samples which were success. We will take a large number of random samples from the population, where the population is generated from a binomial distribution with 10 trials and probability of success \\(p=0.25\\) for each trial.\n\nn_samples &lt;- 10000\nsample_size &lt;- 100 # each sample will be of size 100\nn_trials &lt;- 10\nsample_proportions &lt;- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  rbinom(n = sample_size, size= n_trials, \n                     prob = 0.25) # generate a new sample\n  sample_proportions[i] = mean(sample_i)/n_trials # obtain proportion for each sample\n}\n\n\n\nShow Code\n\n\nsampling_dist_prop &lt;- ggplot(data.frame(sample_proportions),\n                             aes(sample_proportions))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution',\n       x = 'Sample proportions',y = '')\n\n\n\n\n\n\n\nThe plot above is the distribution of sample proportions after taking 10,000 samples with size \\(n=100\\) from the population. The overall distribution appears to be normally distributed (bell shaped curve).\nThrough the CLT, the sampling distribution for the sample proportions will be distributed with mean \\(p\\) and standard error \\(\\sqrt{\\frac{p(1-p)}{n}}\\), \\(\\hat{p} \\sim N(p,\\sqrt{\\frac{p(1-p)}{n}} )\\)\n\nmean(sample_proportions)\n\n#&gt; [1] 0.249721\n\n\nwhich is approximately the population parameter \\(p=0.25\\).\nThe standard error is\n\nsd(sample_proportions)\n\n#&gt; [1] 0.01359789\n\n\n\n\nApplication: Sample proportion\nUsing the penguins dataset we will generate a sampling distribution for the sample proportion of the Adelie penguins\nFor simplicity, we will remove any missing observations\n\nspecies &lt;- penguins$species[!is.na(penguins$species)]\n\nRecall, to obtain the sampling distribution for the sample proportion\nFor the total number of samples do the following:\n\nGenerate a random sample of size \\(n\\) from the population, penguins$species (without missing observations)\nCompute the sample proportion of the penguins Adelie species and store the result in a variable, say adelie_proportions\nPlot each of the adelie_proportions using a histogram\n\n\nn_samples &lt;- 10000\nsample_size &lt;- 50\n\nadelie_proportions &lt;- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  sample(species, size = sample_size) # generate a new sample from the population\n  adelie_proportions[i] = mean(sample_i == 'Adelie') # obtain proportion for each sample\n}\n\n\n\nShow Code\n\n\nsampling_dist &lt;- ggplot(data.frame(adelie_proportions),\n                        aes(adelie_proportions))+\n  geom_bar(fill = 'steelblue',alpha = 0.3,\n                 color = 'black')+\n  labs(title = 'Sampling distribution of Sample Proportion',\n       subtitle = \"Adelie penguins\",\n       x = 'Sample proportions',y = '')\n\n\n\n\n\n\n\nThe mean for the sampling distribution is\n\nmean(adelie_proportions)\n\n#&gt; [1] 0.441898\n\n\nwhich is approximately the population proportion for the Adelie penguins (without missing observations)\n\nprop.table(table(species))['Adelie']\n\n#&gt;    Adelie \n#&gt; 0.4418605\n\n\nthe standard error for the sampling distribution is\n\nsd(adelie_proportions)\n\n#&gt; [1] 0.06489606\n\n\nwhich should approximately be \\(\\sqrt{\\frac{p(1-p)}{n}}\\)\n\np &lt;- prop.table(table(species))['Adelie']\nsqrt((p*(1-p))/sample_size)\n\n#&gt;     Adelie \n#&gt; 0.07023102"
  },
  {
    "objectID": "notes/13-simulation_test_of_significance.html",
    "href": "notes/13-simulation_test_of_significance.html",
    "title": "Simulation-Based Test of Significance",
    "section": "",
    "text": "Required Packages\n\n\nlibrary(openintro)\nlibrary(palmerpenguins)\nlibrary(mosaic)\nlibrary(ggplot2)\n\n\ntheme_set(theme_bw())\ntheme_replace(panel.grid.minor = element_blank(),\n              panel.grid.major = element_blank())"
  },
  {
    "objectID": "notes/13-simulation_test_of_significance.html#comparing-two-proportions",
    "href": "notes/13-simulation_test_of_significance.html#comparing-two-proportions",
    "title": "Simulation-Based Test of Significance",
    "section": "Comparing two proportions",
    "text": "Comparing two proportions\nFrom the openintro package we consider the sten30 dataset. An experiment that studies effectiveness of stents in treating patients at risk of stroke with some unexpected results, this dataset represent the results 30 days after stroke\n\nstent30 &lt;- openintro::stent30\n\n\n\n#&gt; Rows: 451\n#&gt; Columns: 2\n#&gt; $ group   &lt;fct&gt; treatment, treatment, treatment, treatment, treatment, treatme…\n#&gt; $ outcome &lt;fct&gt; stroke, stroke, stroke, stroke, stroke, stroke, stroke, stroke…\n\n\nWe are interesting in testing if the probability of having a stroke is different for those in the control and treatment group. The null and alternative hypotheses are given by \\[\n\\begin{align*}\nH_0: \\pi_1 &= \\pi_2 \\\\[5pt]\nH_a: \\pi_1 &\\neq \\pi_2\n\\end{align*}\n\\]\n\nobserved_tbl &lt;- tally(outcome~group,data=stent30 ,format='proportion')\n\nThe observed statistic is the difference in proportion of individuals who did get a stroke under the treatment and control groups\n\nobserved_diff &lt;- observed_tbl['stroke','treatment'] - observed_tbl['stroke','control']\n\n\nobserved_diff\n\n#&gt; [1] 0.09005271\n\n\nBelow is a for loop that will simulate the null distribution for our test statistic we will use to test our hypothesis\n\nset.seed(234)\niters &lt;- 5000\ndiff_proportions &lt;- numeric(iters)\n\nfor(i in 1:iters){\n  group_shuffle_i &lt;- sample(stent30$group)\n  cond_props_i &lt;- tally(stent30$outcome~group_shuffle_i,format = 'proportion')\n  \n  diff_proportions[i] &lt;- cond_props_i['stroke','treatment'] - cond_props_i['stroke','control']\n}\n\n\n\nShow Code\n\n\nsampling_dist &lt;- ggplot(data.frame(diff_proportions),\n                        aes(diff_proportions))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=25)+\n  geom_vline(xintercept = observed_diff,linetype = 2,\n             color ='red')+\n  geom_vline(xintercept = -observed_diff,linetype = 2,\n             color ='red')+\n  labs(title = 'Sampling distribution',\n       subtitle = '',\n       x = 'Sample Difference of Proportions',y = '')\n\n\n\n\n\n\n\nUsing our null distribution, we can decide on the strength of evidence for or against the null hypothesis. The simulation-based \\(p\\)-value is then computed as\n\nmean(abs(diff_proportions) &gt;= observed_diff)\n\n#&gt; [1] 8e-04\n\n\nUsing a significance level of \\(\\alpha=0.05\\), we have enough evidence in favor of the alternative hypothesis. That is, we reject the null hypothesis and suggest there is a difference in proportions of patients getting a stroke under the control and treatment group."
  },
  {
    "objectID": "notes/13-simulation_test_of_significance.html#comparing-two-means",
    "href": "notes/13-simulation_test_of_significance.html#comparing-two-means",
    "title": "Simulation-Based Test of Significance",
    "section": "Comparing two means",
    "text": "Comparing two means\nUsing the penguins dataset from the package palmerpenguins, in this portion of the tutorial we are interested in whether the average body mass (in grams) of Adelie type penguins is less than that of Chinstrap penguins\nWe will test this comparing the mean body mass between Adelie and Chinstrap penguins using a simulation-based approach\nFirst, we remove missing data concerning the two variables of interest species and body_mass_g\n\nsubset_filter &lt;- is.na(penguins$species)|is.na(penguins$body_mass_g)| penguins$species == 'Gentoo'\n\npenguins_dat will be the dataset without missing values for the species and body_mass_g variables\n\npenguins_dat &lt;- subset(penguins,!subset_filter)\n\n\nobserved_means &lt;- mean(body_mass_g ~ species, data = penguins_dat)\n\nThe observed statistic is the difference in means of Adelie type penguins and Chinstrap penguins\n\nobserved_means_diff &lt;- observed_means['Adelie']-observed_means['Chinstrap']\nobserved_means_diff\n\n#&gt;    Adelie \n#&gt; -32.42598\n\n\nThe null and alternative hypotheses are given by\n\\[\n\\begin{align*}\nH_0: \\mu_{\\text{adelie}} = \\mu_{\\text{Chinstrap}} \\\\\nH_a:  \\mu_{\\text{adelie}} &lt; \\mu_{\\text{Chinstrap}}\n\\end{align*}\n\\]\nBelow is a for loop that will simulate the null distribution for our test statistic we will use to test our hypothesis\n\niters &lt;- 5000\ndiff_means &lt;- numeric(iters) \nset.seed(123)\n\nfor (i in 1:iters){\n    species_shuffle_i &lt;- sample(penguins_dat$species)\n    means_i &lt;- mean(body_mass_g ~ species_shuffle_i, data=penguins_dat)\n    diff_means[i] &lt;- means_i[1] - means_i[2]\n}\n\n\n\nShow Code\n\n\nsampling_dist &lt;- ggplot(data.frame(diff_means),\n                        aes(diff_means))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  geom_vline(xintercept = observed_means_diff,linetype = 2,\n             color ='red')+\n  labs(title = 'Sampling distribution',\n       subtitle = '',\n       x = 'Sample Difference of Means',y = '')\n\n\n\n\n\n\n\nUsing our null distribution, we can decide on the strength of evidence for or against the null hypothesis. The simulation-based \\(p\\)-value is then computed as\n\nmean(diff_means &lt;= observed_means_diff)\n\n#&gt; [1] 0.3098\n\n\nUsing a significance level of \\(\\alpha=0.05\\), we do not have enough evidence in favor of the alternative hypothesis. That is, we fail to reject the null hypothesis and cant suggest the average body mass of Adelie type penguins is less than that of a Chinstrap penguin"
  },
  {
    "objectID": "notes/02-importing_data.html",
    "href": "notes/02-importing_data.html",
    "title": "Importing Data in R",
    "section": "",
    "text": "Warning\n\n\n\nThis tutorial assumes the data set is in working condition. That is we assume the default settings for read.csv. In some cases we may need to change the header, specify the field separator and more. See ?read.csv for further details and examples.\n\n\nWe will now import a csv file, to do this we will use the read.csv function. A simple template to follow is\n\nread.csv(file = 'path where csv is located in your computer')\n\nAn easy way to find the location of your data (or any file) is using the file.choose() function in R. file.choose() will bring up a file explorer window that allows you to interactively choose a file path to work with.\nIn your console, run the following command\n\nfile.choose()\n\nFor example, after running the above command the births dataset is located in\n\n\n[1] \"/Users/toledo60/Desktop/Projects/ucla-stats13/data/births.csv\"\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your location and operating system the directory will be different\n\n\nTherefore, to read the births data set I would copy/paste the directory of the csv location and run the following command\n\nbirth_dat &lt;- read.csv(file = \"/Users/toledo60/Desktop/Projects/ucla-stats13/data/births.csv\")\n\n\n\n  X Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits   Marital\n1 1   Male     No    116      9   28   34     6     3       2     10   Married\n2 2   Male     No    126      8   30   18    12    12       1     14 Unmarried\n3 3   Male     No    161      8   28   29    12    12       3     14   Married\n4 4   Male     No    133      9   26   23     8     9       3     10   Married\n5 5 Female     No    119      8   30   19    12    12       2     12 Unmarried\n6 6   Male     No    110      9   30   26    12    16       2     13 Unmarried\n  Racemom Racedad   Hispmom   Hispdad Gained     Habit MomPriorCond BirthDef\n1   White   White   Mexican   Mexican     30 NonSmoker         None     None\n2   White Unknown   NotHisp   Unknown     50    Smoker At Least One     None\n3   White   White OtherHisp OtherHisp     65 NonSmoker         None     None\n4   White   White   Mexican   Mexican      8 NonSmoker         None     None\n5   Black Unknown   NotHisp   Unknown     20 NonSmoker         None     None\n6   Black Unknown   NotHisp   Unknown     32 NonSmoker         None     None\n     DelivComp BirthComp\n1         None      None\n2         None      None\n3 At Least One      None\n4 At Least One      None\n5         None      None\n6         None      None\n\n\nWe are not just limited to csv files, we can import data from Excel (in csv, XLSX, or txt format), SAS, Stata, SPSS, or others. A good reference to import various data formats can be found on datacamp r-data-import tutorial"
  },
  {
    "objectID": "notes/02-importing_data.html#comma-seperated-values-csv",
    "href": "notes/02-importing_data.html#comma-seperated-values-csv",
    "title": "Importing Data in R",
    "section": "",
    "text": "Warning\n\n\n\nThis tutorial assumes the data set is in working condition. That is we assume the default settings for read.csv. In some cases we may need to change the header, specify the field separator and more. See ?read.csv for further details and examples.\n\n\nWe will now import a csv file, to do this we will use the read.csv function. A simple template to follow is\n\nread.csv(file = 'path where csv is located in your computer')\n\nAn easy way to find the location of your data (or any file) is using the file.choose() function in R. file.choose() will bring up a file explorer window that allows you to interactively choose a file path to work with.\nIn your console, run the following command\n\nfile.choose()\n\nFor example, after running the above command the births dataset is located in\n\n\n[1] \"/Users/toledo60/Desktop/Projects/ucla-stats13/data/births.csv\"\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDepending on your location and operating system the directory will be different\n\n\nTherefore, to read the births data set I would copy/paste the directory of the csv location and run the following command\n\nbirth_dat &lt;- read.csv(file = \"/Users/toledo60/Desktop/Projects/ucla-stats13/data/births.csv\")\n\n\n\n  X Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits   Marital\n1 1   Male     No    116      9   28   34     6     3       2     10   Married\n2 2   Male     No    126      8   30   18    12    12       1     14 Unmarried\n3 3   Male     No    161      8   28   29    12    12       3     14   Married\n4 4   Male     No    133      9   26   23     8     9       3     10   Married\n5 5 Female     No    119      8   30   19    12    12       2     12 Unmarried\n6 6   Male     No    110      9   30   26    12    16       2     13 Unmarried\n  Racemom Racedad   Hispmom   Hispdad Gained     Habit MomPriorCond BirthDef\n1   White   White   Mexican   Mexican     30 NonSmoker         None     None\n2   White Unknown   NotHisp   Unknown     50    Smoker At Least One     None\n3   White   White OtherHisp OtherHisp     65 NonSmoker         None     None\n4   White   White   Mexican   Mexican      8 NonSmoker         None     None\n5   Black Unknown   NotHisp   Unknown     20 NonSmoker         None     None\n6   Black Unknown   NotHisp   Unknown     32 NonSmoker         None     None\n     DelivComp BirthComp\n1         None      None\n2         None      None\n3 At Least One      None\n4 At Least One      None\n5         None      None\n6         None      None\n\n\nWe are not just limited to csv files, we can import data from Excel (in csv, XLSX, or txt format), SAS, Stata, SPSS, or others. A good reference to import various data formats can be found on datacamp r-data-import tutorial"
  },
  {
    "objectID": "notes/02-importing_data.html#text-file-txt",
    "href": "notes/02-importing_data.html#text-file-txt",
    "title": "Importing Data in R",
    "section": "Text File (TXT)",
    "text": "Text File (TXT)\nNext, we consider importing a .txt file. To do so we will use the read.table function instead of the read.csv function. For this example, we consider the ozone.txt file from our course website\nA simple template to follow is\n\nread.table(file = 'path where txt file is located in your computer')\n\nAfter running file.choose() on our console and locating the path in which we stored our data\n\nfile.choose()\n\n\n\n[1] \"/Users/toledo60/Desktop/Projects/ucla-stats13/ucla/stats10/data/ozone.txt\"\n\n\nwe can copy/paste the path as follows\n\nozone_dat &lt;- read.table(file = \"/Users/toledo60/Desktop/Projects/ucla-stats13/data/ozone.txt\", \n                        header =TRUE)\n\n\nozone_dat\n\n\n\n          x       y    o3\n1 -120.0258 34.4622 0.044\n2 -119.7413 36.7055 0.081\n3 -121.7333 36.4819 0.035\n4 -119.2908 36.3325 0.080\n5 -117.1289 32.8364 0.053\n\n\nYou will notice we now used an additional argument header = TRUE in our read.table function. We use header=TRUE, whenever the text tile contains names of the variables as its first line.\nIf we forget to use header=TRUE, the first line of the text file will be treated as a row of the dataset and read.table will automatically create the variable names for us\n\nwrong_ozone_dat &lt;- read.table(file = \"/Users/toledo60/Desktop/Projects/ucla-stats13/data/ozone.txt\")\n\n\nwrong_ozone_dat\n\n\n\n         V1      V2    V3\n1         x       y    o3\n2 -120.0258 34.4622 0.044\n3 -119.7413 36.7055 0.081\n4 -121.7333 36.4819 0.035\n5 -119.2908 36.3325  0.08\n\n\nIn the above example, read.table automatically create the variable names V1,V2,V3 for each column and the first row has values x,y,o3 (which is incorrect).\nIn conclusion, some text files do not have variable names in the first row and only contain the actual data. As a result, it is our responsibility to import the data in a suitable manner."
  },
  {
    "objectID": "notes/14-simple_linear_regression.html",
    "href": "notes/14-simple_linear_regression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Required Packages\n\n\nlibrary(palmerpenguins)\nlibrary(mosaic)\nlibrary(ggplot2)\n\n\ntheme_set(theme_bw())\ntheme_replace(panel.grid.minor = element_blank(),\n              panel.grid.major = element_blank())"
  },
  {
    "objectID": "notes/14-simple_linear_regression.html#introduction-to-simple-linear-regression",
    "href": "notes/14-simple_linear_regression.html#introduction-to-simple-linear-regression",
    "title": "Simple Linear Regression",
    "section": "Introduction to Simple Linear Regression",
    "text": "Introduction to Simple Linear Regression\nSimple linear regression is a statistical method which allows us to predict a quantitative outcome \\(y\\) on the basis of a single predictor variable \\(x\\).\n\nSometimes \\(x\\) is regarded as the predictor, explanatory, or independent variable.\nSimilarly, \\(y\\) is regarded as the response, outcome, or dependent variable\n\nWe will only use predictor and response to denote our variables of interest. The goal is to define a statistical model that defines the response variable (y) as a function of the predictor (x) variable. Once, we built a statistically significant model, it’s possible to use it for predicting outcomes on the basis of new predictor values.\nBelow are all the numerical variables in our dataset\n\ncolnames(Filter(is.numeric,penguins))\n\n[1] \"bill_length_mm\"    \"bill_depth_mm\"     \"flipper_length_mm\"\n[4] \"body_mass_g\"       \"year\"             \n\n\nFor our simple linear regression model, we will consider the body mass (grams) of the penguins as the response variable and the length of the length of the the penguin’s flipper (millimeters) as our single predictor variable\nOur model takes the form \\[\n(body\\_mass_g) = \\beta_0 + \\beta_1 \\cdot (flipper\\_length_{mm}) + \\epsilon\n\\] where\n\n\\(\\beta_0\\) is the intercept of the regression line, (when flipper_length_mm =0)\n\\(\\beta_1\\) is the slope of the regression line\n\\(\\epsilon\\) is the error terms\n\nFor simplicity we will remove any missing data from our variables of interest\n\nmissing_dat &lt;- is.na(penguins$flipper_length_mm)|is.na(penguins$body_mass_g)\n\npenguins_dat will be the dataset without missing values for the flipper_length_mm and body_mass_g variables\n\npenguins_dat &lt;- subset(penguins,!missing_dat)\n\nFor syntax purposes, we create variables for flipper_length_mm and body_mass_g to avoid using penguins_dat$ throughout the tutorial\n\nflipper_length_mm &lt;- penguins_dat$flipper_length_mm\nbody_mass_g &lt;- penguins_dat$body_mass_g\n\nWe can view the relationship between body_mass_g and flipper_length_mm using a scatter plot\n\nplot(flipper_length_mm,body_mass_g,\n     xlab = 'Flipper length (mm)',\n     ylab = 'Body mass (grams)')"
  },
  {
    "objectID": "notes/14-simple_linear_regression.html#slr-in-r",
    "href": "notes/14-simple_linear_regression.html#slr-in-r",
    "title": "Simple Linear Regression",
    "section": "SLR in R",
    "text": "SLR in R\n\nsimple_lm &lt;- lm(body_mass_g ~ flipper_length_mm)\nsimple_lm\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm)\n\nCoefficients:\n      (Intercept)  flipper_length_mm  \n         -5780.83              49.69  \n\n\nThe model can be written as body_mass_g = -5780.83 + 49.69*flipper_length_mm, this is our line of best fit. We can then plot our line of best fit as shown below\n\nplot(flipper_length_mm,body_mass_g,\n     xlab = 'Flipper length (mm)',\n     ylab = 'Body mass (grams)')\nabline(simple_lm,col = 'red',lwd =2)\n\n\n\n\nMoreover, we can use the summary() function to quickly check whether our predictor is significantly associated with the response variable. If so, we can further assess how well our model fits our actual data by utilizing metrics provided by the summary output\n\nModel Summary\n\nlm_summary &lt;- summary(simple_lm)\nlm_summary\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1058.80  -259.27   -26.88   247.33  1288.69 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -5780.831    305.815  -18.90   &lt;2e-16 ***\nflipper_length_mm    49.686      1.518   32.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.3 on 340 degrees of freedom\nMultiple R-squared:  0.759, Adjusted R-squared:  0.7583 \nF-statistic:  1071 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\n\nThe ’*’ symbols indicate the level of significance for the respective term. The significance codes below the line shows the definitions for the level of significance; one star means \\(0.01 &lt; p &lt; 0.05\\), two stars mean \\(0.001 &lt; p &lt; 0.01\\), and similar interpretations for the remaining symbols\nBreaking down the summary output, it contains the following components (Extract using $ operator)\n\n\nCall\nThe function (formula) used to compute the regression model\n\nlm_summary$call\n\nlm(formula = body_mass_g ~ flipper_length_mm)\n\n\n\n\nResiduals\nThe difference between predicted values for the response variable using our constructed model and the observed response values from our original data. Mathematically, \\[\nr_i = y_i - \\hat{y}_i\n\\]\nIf the residual is\n\nPositive: The predicted value was an underestimate of the observed response\nNegative: The predicted value was an overestimate of the observed response\nZero: The predicted value is exactly the same as the observed response\n\n\n\nShow Code\n\n\ndat &lt;- cbind.data.frame(flipper_length_mm,body_mass_g)\n\np1 &lt;- ggplot(dat, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_smooth(method = \"lm\", se = FALSE, \n              color = \"lightgrey\",formula = 'y~x') +\n  geom_segment(aes(xend = flipper_length_mm, \n                   yend = predict(simple_lm)), alpha = .2) +\n  geom_point(aes(color=residuals(simple_lm))) +\n  guides(color = 'none') + \n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  labs(x = 'Flipper length (mm)',\n       y = 'Body mass (grams)')+\n  theme_bw()+\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major = element_blank())\n\n\n\n\n\n\n\nThe figure above shows the magnitude of the resulting residuals from our fitted model. The darker the red points the more positive the residuals were, the darker the blue points the more negative the residuals were.\nAssuming our line is the “line of best fit” the sum of the residuals always equals zero\n\nsum(lm_summary$residuals)\n\n[1] 6.878054e-12\n\n\nA few plots of the residuals to assess our regression assumptions:\nResiduals vs fitted values plot is used to detect non-linearity, unequal error variances, and possible outliers\nThe residuals should form a “horizontal band” around the y=0 line, suggesting the assumption that the relationship is linear is reasonable as well that the variances of the error terms are equal\n\nplot(fitted(simple_lm),simple_lm$residuals,\n     xlab = 'Fitted values',\n     ylab = 'Residuals',\n     main = 'Residuals vs Fitted Values')\nabline(a = 0, b = 0, col = 'red', lwd = 2)\n\n\n\n\nThe following histogram of residuals suggests the residuals (and hence the error terms) are normally distributed\n\n\nShow Code\n\n\nlm_residuals &lt;- simple_lm$residuals\nh &lt;- hist(lm_residuals,\n          main = 'Distribution of Residuals',\n          xlab = 'Residuals')\nxfit &lt;- seq(min(lm_residuals), max(lm_residuals), length = 40) \nyfit &lt;- dnorm(xfit, mean = mean(lm_residuals), sd = sd(lm_residuals))\nyfit &lt;- yfit * diff(h$mids[1:2]) * length(lm_residuals) \nlines(xfit, yfit, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nA more commonly used plot to test the normality of our residuals, is using a so-called Q-Q plot\n\nqqnorm(simple_lm$residuals)\nqqline(simple_lm$residuals) \n\n\n\n\nTo determine normality, if the residuals in the plot fall along the plotted line, then the data is normally distributed\n\n\nCoefficients\nEstimated regression beta coefficients \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\), alongside their standard errors, \\(t\\)-test, and \\(p\\)-values.\n\nlm_summary$coefficients\n\n                     Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept)       -5780.83136 305.814504 -18.90306  5.587301e-55\nflipper_length_mm    49.68557   1.518404  32.72223 4.370681e-107\n\n\nFrom the output above:\n\nthe model can be written as body_mass_g = -5780.831 + 49.686*flipper_length_mm\nthe intercept \\(\\beta_0\\) is -5780.831. It can be interpreted as the predicted body mass in grams for a flipper length of zero millimeters. In most scenarios the interpretation of the intercept coefficient will not make sense. For example, when the length of the flipper is zero millimeters we predict the weight of the penguin will be about -5780 grams\nthe regression beta coefficient for the variable flipper_length_mm \\(\\beta_1\\) is 49.686. That is, for each additional millimeter of the flipper we expect the penguin to gain 49.686 grams of body mass. For example, if the length of the flipper is 200 millimeters then we expect the penguin’s body mass to be about -5780.831 + 49.686 *200 = 4156.369 grams\n\n\n\nModel Performance\nResidual standard error (RSE), R-squared, Adjusted R-squared, and the F-statistic are metrics used to check our model performance. That is, how well does our model fit the data.\n\nmetrics &lt;- c(lm_summary$sigma,lm_summary$r.squared,\n             lm_summary$adj.r.squared,\n             lm_summary$fstatistic['value'])\n\nnames(metrics) &lt;- c('RSE','R_squared','Adj_R_squared','F_statistic')\n\n\nmetrics\n\n          RSE     R_squared Adj_R_squared   F_statistic \n  394.2781775     0.7589925     0.7582837  1070.7445922 \n\n\n\nResidual standard error\nThe average variation of the observations points around the fitted regression line. This is the standard deviation of residual errors The closer to this value is to zero the better.\n\n\nR-squared/Adjusted R-squared\nThe proportion of information (i.e. variation) in the data that can be explained by the model. The adjusted R-squared adjusts for the degrees of freedom. The higher these values are the better.\nIn the simple linear regression setting R-squared is the square of the Pearson correlation coefficient\n\ncor(flipper_length_mm,body_mass_g)^2\n\n[1] 0.7589925\n\n\n\n\nF-statistic\nThe F-statistic gives the overall significance of the model. It assess whether at least one predictor variable has a non-zero coefficient. The higher this value the better. However, this test only becomes more important when dealing with multiple predictors instead of a single predictor found in a simple linear regression."
  },
  {
    "objectID": "notes/14-simple_linear_regression.html#test-of-significance-for-the-slope",
    "href": "notes/14-simple_linear_regression.html#test-of-significance-for-the-slope",
    "title": "Simple Linear Regression",
    "section": "Test of Significance for the Slope",
    "text": "Test of Significance for the Slope\n\nTheory-based approach\n\n\nSimulation-based approach"
  },
  {
    "objectID": "notes/03-intro_numerical_variables.html",
    "href": "notes/03-intro_numerical_variables.html",
    "title": "Summarizing Numerical Data",
    "section": "",
    "text": "We will use the births data set to summarize and visualize numerical variables using the base R approach\nNumerical data refers to data that can be measured and expressed as a number, such as age, height, weight, and temperature. Numerical data can be discrete or continuous"
  },
  {
    "objectID": "notes/03-intro_numerical_variables.html#single-numerical-variable",
    "href": "notes/03-intro_numerical_variables.html#single-numerical-variable",
    "title": "Summarizing Numerical Data",
    "section": "Single numerical variable",
    "text": "Single numerical variable\nOne way to extract all the numerical columns is using both Filter and is.numeric functions. Below are the first five rows of all the numerical columns in birth_dat\n\nFilter(is.numeric,birth_dat)\n\n\n\n  X weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits Gained\n1 1    116      9   28   34     6     3       2     10     30\n2 2    126      8   30   18    12    12       1     14     50\n3 3    161      8   28   29    12    12       3     14     65\n4 4    133      9   26   23     8     9       3     10      8\n5 5    119      8   30   19    12    12       2     12     20\n\n\nThe names of the numerical columns can be obtained using colnames() function in combination with the above statement\n\ncolnames(Filter(is.numeric,birth_dat) )\n\n [1] \"X\"       \"weight\"  \"Apgar1\"  \"Fage\"    \"Mage\"    \"Feduc\"   \"Meduc\"  \n [8] \"TotPreg\" \"Visits\"  \"Gained\" \n\n\nWe will only consider the weight variable from our dataset to demonstrate methods to summarize and visualize a numerical variable.\nFunctions for numerical summaries include, but not limited to,\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nmean()\nmean\n\n\nmedian()\nmedian\n\n\nmode()\nmode\n\n\nsd()\nstandard deviation\n\n\nvar()\nvariance\n\n\nmin()\nminimum\n\n\nmax()\nmaximum\n\n\nsummary()\nComputes the following: Minimum ,1st Quartile, Median,Mean ,3rd Quartile,Maximum\n\n\n\nNext, we’ll save the values from weight column into a separate variable and compute several numerical summaries listed above\n\nbirth_weight &lt;- birth_dat$weight\n\n\nmean(birth_weight)\n\n[1] 116.0536\n\nmedian(birth_weight)\n\n[1] 117\n\nmin(birth_weight)\n\n[1] 14\n\nmax(birth_weight)\n\n[1] 177\n\n\n\nsummary(birth_weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   14.0   106.0   117.0   116.1   129.0   177.0 \n\n\nWhile summary() give us a quick numerical summary of our distribution, it is important to also visualize the overall distribution using a plot such as a boxplot\n\nboxplot(birth_weight)\n\n\n\n\nor a histogram\n\nhist(birth_weight)\n\n\n\n\nFrom the histogram above, while the overall distribution of the birth weights is symmetrical there are outliers causing the distribution to be skewed to the left.\nFor boxplot() and hist() we used the default settings, while they are informative we can alter their appearance to be more professional.\nFor example, we changed the x-axis label and y-axis label using xlab,ylab arguments,respectively. We changed the title with main and the color of the boxplot with col. The col argument can take values such as red,blue or any HEX code, see ?boxplot for further customization.\n\nboxplot(birth_weight,\n        main='Boxplot of Birth Weights',\n        xlab ='birth weights', ylab='ounces',\n        col='#61b1ed')\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSearch ‘color picker’ in Google and copy/paste the generated hex code\n\n\nWe can apply similar customization to our histogram\n\nhist(birth_weight,\n     main='Histogram of Birth Weights',\n     xlab ='birth weights',\n     col='#d1584f',\n     breaks=20)\n\n\n\n\nThe hist function uses the Sturges method by default to determine the number of breaks on the histogram. We can manually change the number of breaks, but we should be careful not to specify a low or high number of breaks. Usually the default setting is appropriate for most scenarios."
  },
  {
    "objectID": "notes/03-intro_numerical_variables.html#two-numerical-variables",
    "href": "notes/03-intro_numerical_variables.html#two-numerical-variables",
    "title": "Summarizing Numerical Data",
    "section": "Two numerical variables",
    "text": "Two numerical variables\nWe consider the following two numerical variables: Feduc and Meduc, which is the highest education for fathers and mothers in this dataset, respectively.\n\nfather_eduction &lt;- birth_dat$Feduc\nmother_education &lt;- birth_dat$Meduc\n\nWe can compare their distributions in a single plot as we did in Section 2 with boxplots\n\nboxplot(father_eduction,mother_education,\n        names = c('father','mother'),\n        col=c('#f5d376','#f5767c'),\n        main = 'Highest Education for Parernts',\n        ylab = 'Year')\n\n\n\n\nFor boxplots its pretty straight forward to compare two numerical distributions using the syntax boxplot(v1,v2,...). For histograms it requires a bit more work.\nWe start by creating a histogram for the first variable, then creating another histogram for the second variable but using the argument add=TRUE. We must specify a unique color for each histogram representing the variables. In order for both of the histograms to fit properly on the same plot we must take into account the lowest and highest values among the multiple numerical variables.\n\nlow_x &lt;- min(father_eduction,mother_education)\nhigh_x &lt;- max(father_eduction,mother_education)\n\nLastly, we must specify a legend to appropriately distinguish the multiple histograms using the function legend().\n\nhist(father_eduction, col='lightcoral',\n     xlim=c(low_x,high_x),\n     main='Education for Parents', xlab='Year')\nhist(mother_education, col='lightblue', add=TRUE)\nlegend('topright', legend = c('father', 'mother'), \n       fill=c('lightcoral', 'lightblue'))\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using legend() it is important that you specify the correct ordering of colors for each group, otherwise the legend would be incorrect. For example, in our first histogram we chose “lightcoral” to represent “father” and “lightblue” to represent “mother” distributions. Which is why we used the arguments: legend=c('father','mother'), fill = c('lightcoral','lightblue') in that order\n\n\nWe can also consider a scatter plot to visualize the relationship between two numerical variables. We consider the two numerical variables Gained and weight. Gained describes the weight gained during the pregnancy term and weight describe the weight of the baby at birth.\n\nplot(x = birth_dat$Gained,y = birth_dat$weight,\n     main = 'Baby weight vs pregnancy weight gain',\n     xlab = 'weight gained during pregnancy',\n     ylab = 'Baby weight (oz.)',\n     col='lightcoral')"
  },
  {
    "objectID": "notes/12-theory_test_of_significance.html",
    "href": "notes/12-theory_test_of_significance.html",
    "title": "Theory-Based Test of Significance",
    "section": "",
    "text": "Required Packages\n\n\nlibrary(openintro)"
  },
  {
    "objectID": "notes/12-theory_test_of_significance.html#hypothesis-tests",
    "href": "notes/12-theory_test_of_significance.html#hypothesis-tests",
    "title": "Theory-Based Test of Significance",
    "section": "Hypothesis tests",
    "text": "Hypothesis tests\nThere are a few components common to every hypothesis test\nNull Hypothesis\nA hypothesis that posits there is no significant difference, effect, or relationship in the variables being studied. Suggesting any observed differences are due to random chance Denoted by \\(H_0\\) (“H naught”)\nAlternative Hypothesis\nA hypothesis that contradicts the null hypothesis, proposing that there is a significant difference, effect, or relationship in the variables being studied, beyond what could be explained by random chance. Denoted by \\(H_a\\)\nTest Statistic\nA numerical value calculated from sample data used to assess the strength of the evidence against the null hypothesis, with larger values indicating stronger evidence in favor of the alternative hypothesis\np-value\nThe probability of a test statistic as rare or even more rare than the one observed under the assumptions of the null hypothesis\nThe p-value quantifies the likelihood of observing the data or more extreme results under the null hypothesis; a lower p-value indicates that the observed statistic is highly improbable under the null hypothesis, strengthening the evidence against it and often leading to its rejection in favor of the alternative hypothesis"
  },
  {
    "objectID": "notes/12-theory_test_of_significance.html#one-sample-t-test",
    "href": "notes/12-theory_test_of_significance.html#one-sample-t-test",
    "title": "Theory-Based Test of Significance",
    "section": "One Sample \\(t\\)-test",
    "text": "One Sample \\(t\\)-test\nTo perform a theory-based hypothesis test for the mean of a quantitative variable, following the components of a hypothesis test, but in the context of a one sample \\(t\\)-test\n\nState the null and alternative hypotheses about the mean parameter \\(\\mu\\)\nFrom a sample of data, compute the sample mean \\(\\bar{x}\\) and its standardized \\(t\\)-statistic\nCheck the validity conditions to apply the Central Limit Theorem\nIf the validity conditions hold, compute the \\(p\\)-value by comparing the observed \\(t\\)-statistic to a \\(t\\)-distribution with \\(df = n-1\\)\n\n\nWe can use the t.test() function to conduct this hypothesis test\nThe basic syntax for the t.test() function is as follows:\n\nt.test(x, y = NULL, \n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, conf.level = 0.95, ...)\n\n\nx: numeric vector of data values (our sample)\ny: an optional numeric vector of data values (response)\nalternative: type of hypothesis being performed,\n\n\"two.sided\": \\(H_a: \\mu \\neq \\mu_0\\)\n\"less\": \\(H_a: \\mu &lt; \\mu_0\\)\n\"greater\": \\(H_a: \\mu &gt; \\mu_0\\)\n\nmu: population mean \\(\\mu\\) (null value), can be difference of means if performing a two-sample test\nconf.level: confidence level of the interval \\(100(1-\\alpha)\\)%, where \\(\\alpha\\) is the significance level\n\nFor additional arguments ... see ?t.test()\n\nConsider the Tip Data from opentintro package. The tip data is a simulated data set of tips over a few weeks on a couple days per week. Each tip is associated with a single group, which may include several bills and tables\nFrom this dataset we obtain a simple random sample of 30 tips, but first we only consider tips from bills that were at most $80\n\ntips &lt;- openintro::tips\n\n\n\n#&gt; Rows: 95\n#&gt; Columns: 5\n#&gt; $ week   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#&gt; $ day    &lt;fct&gt; Tuesday, Tuesday, Tuesday, Tuesday, Tuesday, Tuesday, Tuesday, …\n#&gt; $ n_peop &lt;dbl&gt; 0.67, 1.33, 5.33, 2.67, 1.33, 2.00, 1.33, 2.67, 2.00, 1.33, 1.3…\n#&gt; $ bill   &lt;dbl&gt; 17.99, 26.05, 79.49, 32.69, 29.97, 23.91, 18.18, 41.23, 25.19, …\n#&gt; $ tip    &lt;dbl&gt; 2.0000, 4.0000, 14.3082, 5.5000, 6.0000, 7.5000, 3.0600, 8.5000…\n\n\n\ntips_bill_at_most80 &lt;- tips[tips$bill &lt;= 80,]\n\nAmong the tips from bills that were at most $80, we obtained a simple random sample of size 30. We are interested in determining if the average tips throughout the weeks will be greater than $5 per group (for some reason, the business needs on average at least $5 tips per group to stay in business).\nThe null and alternative hypothesis are given by \\[\n\\begin{align*}\nH_0: \\mu = 5 \\\\[5pt]\nH_a: \\mu &gt; 5\n\\end{align*}\n\\]\n\nset.seed(1234)\nrows_sample &lt;- sample(1:nrow(tips_bill_at_most80), 30)\ntips_sample &lt;- tips_bill_at_most80[rows_sample,]\n\nFirst we will carry out a one-sample \\(t\\)-test the long way. First, we compute the sample mean for the tips\n\nx_bar &lt;- mean(tips_sample$tip)\nx_bar\n\n#&gt; [1] 6.533687\n\n\nNext, compute the sample standard deviation of the tips\n\ns &lt;- sd(tips_sample$tip)\n\nthe \\(t\\)-statistic is then computed as\n\\[\nt = \\frac{\\bar{x}-\\mu_0}{\\left(\\frac{s}{\\sqrt{n}} \\right)}\n\\]\n\nn &lt;- length(rows_sample)\nmu_0 &lt;- 5\nt_stat &lt;- (x_bar - mu_0) / (s / sqrt(n))\n\n\nt_stat\n\n#&gt; [1] 1.74364\n\n\nLastly, obtain the \\(p\\)-value\n\np_value &lt;- pt(t_stat, df = n - 1,lower.tail = FALSE)\n\n\np_value\n\n#&gt; [1] 0.0459079\n\n\nAt a significance level of \\(\\alpha=0.05\\), we reject the null hypothesis, indicating there is enough evidence to propose that, on average, the tips per group exceed $5 for bills equal to or less than $80, as supported by the three-week data provided\nWith a single command, we can use t.test() function and obtain all of the following information, without having to compute every step ourselves\n\nobserved sample mean \\(\\bar{x}\\)\n\\(t\\)-statistic\ndegrees of freedom \\((df)\\)\nthe alternative hypothesis we are testing\n\\(p\\)-value\nA 95% confidence interval for \\(\\mu\\)\n\n\nt.test(tips_sample$tip, mu=5, \n       alternative = 'greater')\n\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  tips_sample$tip\n#&gt; t = 1.7436, df = 29, p-value = 0.04591\n#&gt; alternative hypothesis: true mean is greater than 5\n#&gt; 95 percent confidence interval:\n#&gt;  5.039153      Inf\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;  6.533687"
  },
  {
    "objectID": "problem-sets.html",
    "href": "problem-sets.html",
    "title": "Problem Sets",
    "section": "",
    "text": "Chapter P Problems:\n\nP.1.2 (a-e)\nP.1.8 (a-c)\nP.1.10 (a-d)  For part (c), refer to the first graph with men/women  For part (d), refer to second graph with cities\nP.1.16 (a-c)\nP.2.2 (a-b)\nP.2.6\nP.3.5 (a-d)"
  },
  {
    "objectID": "problem-sets.html#homework-1",
    "href": "problem-sets.html#homework-1",
    "title": "Problem Sets",
    "section": "",
    "text": "Chapter P Problems:\n\nP.1.2 (a-e)\nP.1.8 (a-c)\nP.1.10 (a-d)  For part (c), refer to the first graph with men/women  For part (d), refer to second graph with cities\nP.1.16 (a-c)\nP.2.2 (a-b)\nP.2.6\nP.3.5 (a-d)"
  },
  {
    "objectID": "problem-sets.html#homework-2",
    "href": "problem-sets.html#homework-2",
    "title": "Problem Sets",
    "section": "Homework 2",
    "text": "Homework 2\nChapter 1 Problems:\n\n1.1.6 (a-e)\n1.2.16 (a-h)\n1.3.13 (a-f)\n1.4.5 (a-c)"
  },
  {
    "objectID": "problem-sets.html#homework-3",
    "href": "problem-sets.html#homework-3",
    "title": "Problem Sets",
    "section": "Homework 3",
    "text": "Homework 3\nChapter 2 Problems:\n\n2.1.8 (a-d)\n2.1.26 (a-b)\n2.1.28 (a-d)\n2.1.29 (a-b)\n2.1.30 (a-c)\n2.2.6 (a-c)\n2.3.18 (a-c)"
  },
  {
    "objectID": "problem-sets.html#homework-4",
    "href": "problem-sets.html#homework-4",
    "title": "Problem Sets",
    "section": "Homework 4",
    "text": "Homework 4\nChapter 3 Problems:\n\n3.1.18 (a-b)\n3.1.20\n3.1.22 (a-f) for part e, can use either 2SD approach or theory\n3.2.6\n3.2.24\n3.3.15 (a-c)\n3.3.18 (a-f)"
  },
  {
    "objectID": "problem-sets.html#homework-5",
    "href": "problem-sets.html#homework-5",
    "title": "Problem Sets",
    "section": "Homework 5",
    "text": "Homework 5\nChapter 2 Problems:\n\n2.2.25 parts a-d\n\nChapter 3 Problems:\n\n3.3.16 parts a-c\n\nChapter 4 Problems:\n\n4.1.13 (a-c) (no diagram for part c)\n4.2.14 (a-c)\n4.2.16\n4.2.21 (a-f)"
  },
  {
    "objectID": "problem-sets.html#homework-6",
    "href": "problem-sets.html#homework-6",
    "title": "Problem Sets",
    "section": "Homework 6",
    "text": "Homework 6\nChapter 5 Problems:\n\n5.1.6 (a-c)\n5.1.14 (a-d)\n5.1.22 (a-d)\n5.2.12 (a-d)\n5.3.18 (a-g)"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Lecture Notes",
    "section": "",
    "text": "These notes are primarily for STATS 13: Introduction to Statistical Methods for Life and Health Sciences lab discussions. For concepts learned in class, refer to the slides posted on Bruin learn\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nFundamentals of R\n\n\n\n\nImporting Data in R\n\n\n\n\nSummarizing Numerical Data\n\n\n\n\nSummarizing Categorical Data\n\n\n\n\nMOSAIC Library\n\n\n\n\nLogical Statements with Applications\n\n\n\n\nIntroduction to For-Loops\n\n\n\n\nSampling and Simulation\n\n\n\n\nNormal Distribution\n\n\n\n\nIntroduction to Central Limit Theorem\n\n\n\n\nConfidence Intervals\n\n\n\n\nTheory-Based Test of Significance\n\n\n\n\nSimulation-Based Test of Significance\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Lab 4: Centers for Disease Control and Prevention",
    "section": "",
    "text": "Learning Objectives\n\nIncrease skill and comfort with basic data management and syntax in R.\nReview the concepts of the normal distribution.\nReview standardized statistics and introduce \\(z\\)-scores.\nCompute probabilities from the normal distribution and the \\(t\\)-distribution.\nReview the concepts of the \\(t\\)-distribution and the one sample \\(t\\)-test.\nCarry out an entire test of significance for the difference in two proportions using simulations and theory\n\n\n\n\nReadings\nTo prepare for this lab assignment read the following lecture notes\n\nNormal Distribution\nOne Sample \\(t\\)-test\nSimulation-Based Test of Significance\n\nFor reference on past material covering logical statements, visualizing and analyzing categorical/numerical variables refer to\n\nSummarizing and Visualizing Numerical Variables\nSummarizing and Visualizing Categorical Variables\nLogical Statements\n\n\n\n\nRequired Data and Packages\nWe will use two data sets in this lab\n\nWe will use the data from the “College of the Midwest” used in Section 2.1 in the textbook (Introduction to Statistical Investigations by Tintle et al.)\n\n\ncollege &lt;- read.table(\"http://www.isi-stats.com/isi/data/chap3/CollegeMidwest.txt\", \n                      header = TRUE)\n\n\nThe file “cdc.csv” on CCLE contains data based on a random sample conducted by the Centers for Disease Control and Prevention (CDC).\n\nThroughout the lab we will use the mosaic package\n\nlibrary(mosaic)\n\n\n\n\nQuestions\n\nQuestion 1\nCompute the probability of observing a person with a weight of 145 pounds or more from a population that follows a normal distribution with a mean of 127.1 pounds and a standard deviation of 11.7 pounds\n\n\nQuestion 2\nCompute the quantity in Question 1 by first converting the value to a z-score and then calculating the probability with respect to the standard normal distribution. Report both the z-score and the probability\n\n\nQuestion 3\nWhat is the probability of observing a value of 2.3 or more from statistics that follow a t-distribution with 35 degrees of freedom?\n\n\nQuestion 4\nUsing a significance level of \\(\\alpha = 0.01\\), state the conclusion for the hypothesis test in the example above in context\n\n\nQuestion 5\nUse the t.test() function to compute a 99% confidence interval for the mean cumulative GPA of students at the College of the Midwest with the sample taken in the example above\n\n\nQuestion 6\n\nUse either & or | to indicate (by TRUE or FALSE) which entries of ex_vec are greater than 4 AND less than 6.\nUse either & or | to indicate which entries of ex_vec are less than 4 OR greater than 6.\n\n\n\nQuestion 7\nUse the correct notation and the format argument in the tally() function to answer the following questions.\n\nWhat proportion of people in our sample said they are in good health?\nWhat proportion of people in our sample who said they are in good health also said they did not exercise at least once a week?\nWhat proportion of people in our sample said they exercise at least once a week and are in good health?\n\n\n\nQuestion 8\nWhat is the observed statistic that was computed above (i.e., what does obs_diff represent)?\nHint: The obs_diff value is a difference of what two proportions?\n\n\nQuestion 9\nCompute the simulation-based p-value\nHint: The abs() function inputs a numeric vector and outputs the absolute value of each entry. What does mean(abs(diff_props) &gt;= 0.01) compute?\n\n\nQuestion 10\nUsing a significance level of \\(\\alpha = 0.05\\), decide whether or not you would reject the null hypothesis in favor of the alternative. Interpret this decision in context.\n\n\nQuestion 11\n\nCompute the standardized statistic according to the simulation-based approach.\nCompute the standardized statistic according to the theory-based approach. Hint: Note that in the z-score formula, our value that we are standardizing is our observed difference \\(\\hat{p}_1 −\\hat{p}_2\\). The simulation-based and theory-based approaches differ in how the standard error is computed.\n\n\n\nQuestion 12\nCompute the theory-based p-value using the pnorm() function How does this compare to the p-value from Question 9?"
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2: North Carolina Births",
    "section": "",
    "text": "Learning Objectives\nStudents will be able to:\n\nIncrease skill and comfort with basic data management and syntax in R\nCreate statistical graphics to answer questions about a data set\nReview simulating the null distribution for sample proportions\nUnderstand the basic methodology and syntax of the for loop\nCarry out an entire test of significance for one proportion using simulations\n\n\n\n\nReadings\nTo prepare for this lab assignment read the following lecture notes\n\nLogical Statements and Applications\nIntroduction to for-loops\nSampling and Simulation\n\n\n\n\nRequired Data and Packages\nWe will continue to work with the NC data on births used in Lab 1\n\nbirths &lt;- read.csv('your path to births.csv')\n\nThroughout the lab we will use the mosaic package\n\nlibrary(mosaic)\n\n\n\n\nQuestions\n\nQuestion 1\nWhat are the names of the variables that indicate whether or not the mother smoked and the weight of the baby?\n\n\nQuestion 2\nIn addition to the variable that indicates whether or not the mother smoked, choose one numerical (quantitative) variable and one categorical variable that you think might also affect the weight of a baby. Explain why\n\n\nQuestion 3\nCreate a data frame that has only the variables Gender, Premie, weight, and Apgar1, and the observations for which the baby was not premature. Print out the first few rows using the head() function.\n\n\nQuestion 4\nCreate a data frame that has only the four variables you identified in Questions 1 and 2 and the observations for which the mother was a smoker. Print out the first few rows using the head() function\n\n\nQuestion 5\nReport the mean of\n\nthe weights of all babies.\nthe weights of those born to smoking mothers.\nthe weights of those born to non-smoking mothers.\n\nThe sd() function computes the standard deviation of the input. The syntax is identical to that of the mean() function\n\n\nQuestion 6\nReport the standard deviation of\n\nthe weights of all babies\nthe weights of those born to smoking mothers\nthe weights of those born to non-smoking mothers\n\n\n\nQuestion 7\nUsing Births, create two graphics that help answer this question: Does the birth weight of babies born to smoking mothers differ from those born to non-smoking mothers?\nNote: You are not limited to histograms. If you want, you can look into dotPlot(), densityplot(), freqpolygon(), bwplot(), and mosaicplot() for other types of plots to visualize data\n\n\nQuestion 8\nSet the seed to 2023 and simulate tossing 19 coins, where the probability of heads is 0.3 using rflip(). Print the first six lines of the output\n\n\nQuestion 9\nSet the seed to 2023 and simulate 1000 repetitions of tossing 19 coins,where the probability of heads is 0.3. Save the output from this simulation, and print the first six lines from the saved data frame\n\n\nQuestion 10\nSetup your own coin flip simulation. Pick your own seed, sample size, number of repetitions and probability. Print the first six lines of the output.\n\n\nQuestion 11\nSet the seed to 2350 and choose 25 numbers out of the numbers from 1to 1990, without replacement.\n\n\nQuestion 12\nSet the seed to 2350 and simulate 25 coin flips using the sample() function, where the probability of heads (represented by 1) is 0.3. Print the proportion of heads obtained\nHint: What would the mean() function do given the vector of coded coin flips?"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1: Introduction to R",
    "section": "",
    "text": "Learning Objectives\nStudents will be able to:\n\n\n\n\n\n\n\nQuestion 1\n\n\nQuestion 2"
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3: Midwest College",
    "section": "",
    "text": "Learning Objectives\nStudents will be able to:\n\nIncrease skill and comfort with basic data management and syntax in R.\nReview and increase understanding of the basic methodology and syntax of the for loop\nSimulating the sampling distributions for sample proportions and sample means.\nReview the concepts of the sampling distribution and the normal approximation.\nPractice constructing and interpreting confidence intervals.\n\n\n\n\nReadings\nTo prepare for this lab assignment read the following lecture notes\n\nSampling and Simulation\nNormal Distribution\nIntroduction to Central Limit Theorem\nConfidence Intervals\n\n\n\n\nRequired Data and Packages\nWe will use the data from the “College of the Midwest” used in Section 2.1 in the textbook (Introduction to Statistical Investigations by Tintle et al.)\n\ncollege &lt;- read.table(\"http://www.isi-stats.com/isi/data/chap3/CollegeMidwest.txt\", \n                      header = TRUE)\n\nThroughout the lab we will use the mosaic package\n\nlibrary(mosaic)\n\n\n\n\nQuestions\n\nQuestion 1\nSet the seed to 2035 and take a simple random sample of size 30 from the college data frame. Save the random sample of college as a separate R object named sample_0, and print the first six rows to make sure you saved it correctly\nWe will treat sample_0 as our observed sample data from the population dataset college\n\n\nQuestion 2\nCompute the population proportion and sample proportion of students who live on campus, and assign the appropriate symbol to each of the values\n\n\nQuestion 3\nCreate a bar graph that displays the proportions (or relative frequency) of students in your sample from Question 1 who live on campus or not\nHint: Use ?bargraph to find the R documentation for the bargraph() function and find an argument that allows you to change the type of scale (on the y-axis).\n\n\nQuestion 4\nCreate a histogram of the sampling distribution of sample proportions. Superimpose a normal curve by including the argument fit = \"normal\"\n\n\nQuestion 5\nCompute the mean and standard error of the sampling distribution of sample proportions based on your simulation\n\n\nQuestion 6\nVisualize the population distribution and the sample distribution of the cumulative GPA for students at the College of the Midwest. Describe the distributions\nHint: Is the cumulative GPA variable quantitative or categorical?\n\n\nQuestion 7\nCompute the population mean and sample mean of students’ cumulative GPAs, and assign the appropriate symbol to each of the values\n\n\nQuestion 8\nCompute the population standard deviation and sample standard deviation of students’ cumulative GPAs, and assign the appropriate symbol to each of the values\n\n\nQuestion 9\nSet the seed to 9286 and simulate the sampling distribution of the mean cumulative GPA for the students at the College of the Midwest. Simulate the sample means with 1000 random samples of size 30. Provide the code that you use to accomplish this.\n\n\nQuestion 10\nCreate a histogram of the sampling distribution of sample means. Superimpose a normal curve by including the argument fit = \"normal\"\n\n\nQuestion 11\nCompute the mean and standard error of the sampling distribution of sample means based on your simulation\n\n\nQuestion 12\nDo you think the sampling distribution of sample proportions is approximately normal? Be sure to comment on the relevant validity condition(s) and the histogram in Question 4\nHint: Try adjusting the number of histogram bins using the nint argument in the histogram() function.\n\n\nQuestion 13\nAccording to the theory-based method (i.e., normal approximation by invoking the Central Limit Theorem), what is the mean and standard deviation of the sampling distribution of sample proportions? How close are these values to your simulation-based values from Question 5?\n\n\nQuestion 14\nReport and interpret a 95% confidence interval for the proportion of students who live on campus using the theory-based approach. Does this interval contain the population parameter?\n\n\nQuestion 15\nDo you think the sampling distribution of sample means is approximately normal? Be sure to comment on the relevant validity condition(s) and the histogram in Question 10.\n\n\nQuestion 16\nAccording to the theory-based method (i.e., normal approximation by invoking the Central Limit Theorem), what is the mean and standard deviation of the sampling distribution of sample means? How close are these values to your simulation-based values from Question 11?\n\n\nQuestion 17\nReport and interpret a 95% confidence interval for the mean of the cumulative GPA of students using the theory-based approach. The appropriate multiplier can be obtained by running qt(0.975, df = 29). Does this interval contain the population parameter?"
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Lab 5: Body Temperature",
    "section": "",
    "text": "Learning Objectives\n\nAnalyze data consisting of one categorical/binary explanatory and one quantitative response variable\nMake and interpret boxplots\nSimulate the null distribution for the difference in two sample means\nAnalyze data consisting of a quantitative explanatory variable and a quantitative response variable\nMake and interpret scatterplots\nCarry out an entire test of significance for correlation using simulations and theory\n\n\n\n\nReadings\nTo prepare for this lab assignment read the following lecture notes\n\nSimple Linear Regression\n\nFor past reference on visualizing and analyzing categorical/numerical variables refer to\n\nSummarizing and Visualizing Numerical Variables\nSummarizing and Visualizing Categorical Variables\n\n\n\n\nRequired Data and Packages\nWe will use the following data in this lab\n\nThe file \"bodytemp.csv\" on CCLE has data from a study done by Philip A. Mackowiak, MD et al. in 1992 (Mackowiak, et. al, JAMA. 1992;268(12):1578-1580)\n\nThroughout the lab we will use the mosaic package\n\nlibrary(mosaic)\n\n\n\n\nQuestions"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "For both lab assignments and problem sets there is a strict no late assignments policy (unless its a documented reason, such as an emergency)"
  },
  {
    "objectID": "assignments.html#labs",
    "href": "assignments.html#labs",
    "title": "Assignments",
    "section": "Labs",
    "text": "Labs\nLab assignments are an opportunity to put the concepts from the notes into practice to answer questions about a real data set. Your lab report should be turned in as a single pdf file on Bruin Learn\nFor a helpful R reference, see base R, data visualization (ggplot2), and data wrangling (dplyr)\n\n\n\n\n\n\n\n\n\n\nLab 1: Introduction to R\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 2: North Carolina Births\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 3: Midwest College\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 4: Centers for Disease Control and Prevention\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 5: Body Temperature\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignments.html#problem-sets",
    "href": "assignments.html#problem-sets",
    "title": "Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\nProblem sets are nothing more than the worksheets that we do in class. The goal of the problems sets is simply practice: they help you drill the techniques needed to complete the labs.\nThe problem sets can be found here"
  },
  {
    "objectID": "appoitments.html",
    "href": "appoitments.html",
    "title": "Setting up an Appointment",
    "section": "",
    "text": "My appointments typically last under 30 minutes, but if necessary, they can be extended to address further issues. Please note that while I am here to help, I cannot serve as your personal tutor. I encourage you to attend our assigned office hours, ask questions, and work with your classmates to resolve any difficulties you may be experiencing. If you still require assistance or are unable to resolve your concerns, please do not hesitate to schedule an appointment with me"
  },
  {
    "objectID": "appoitments.html#appropriate-email",
    "href": "appoitments.html#appropriate-email",
    "title": "Setting up an Appointment",
    "section": "Appropriate Email",
    "text": "Appropriate Email\nHello TA NAME,\nBriefly explain what you are struggling with and insert screenshots of error messages (If its code related)\nMy available times this week are\n\nMonday: ANYTIME\nTuesday: 10am-12pm, 2-3pm\nWednesday: 10am-1pm\nThursday: ANYTIME\nFriday, 8am-2pm\n\nDo any of these times work for you?\nyour signature"
  },
  {
    "objectID": "appoitments.html#not-appropriate-email",
    "href": "appoitments.html#not-appropriate-email",
    "title": "Setting up an Appointment",
    "section": "Not Appropriate Email",
    "text": "Not Appropriate Email\nHello TA NAME,\nCan we setup an appointment? Im struggling with the assignment\n\nIn the appropriate email example, you have inserted images of your errors and sometimes issues can be resolved through email without even having to meet. If the issue can not be easily resolved through email I now have your schedule and can easily setup an appointment to meet.\nIn the not appropriate email email example, I reply yes then give you my available times, afterwards you reply you can’t meet during those times so you send me your available times, and then I proceed to agree with those times etc… This long and tedious process can be avoided from the start if you just follow the first example :)"
  },
  {
    "objectID": "notes/01-fundamentals.html",
    "href": "notes/01-fundamentals.html",
    "title": "Fundamentals of R",
    "section": "",
    "text": "A variable provides us with named objects that our programs can manipulate. A valid variable name consists of letters, numbers and the dot or underline characters. It is important to note variable names are case sensitive. That is, var1 and Var1 are different variables. Below are appropriate variable names in R\n\n\n\n\n\n\n\nValid Variable Name\nReason\n\n\n\n\nvariable_name\nContains letters and underscore\n\n\nlong.variable_name\nContains letters, dot, and underscore\n\n\nvar\nContains letters\n\n\nvar1\nContains letters and numbers\n\n\nlong.variable_name2\nContains letters, numbers, dot and underscore\n\n\nvar1_name.1\nContains letters, numbers, dot and underscore\n\n\n.var_name\nCan start with period, contains letters and underscore\n\n\n\nThis is a good starting point for valid variable names. Next we demonstrate a few examples where variable names are invalid.\n\n\n\n\n\n\n\nInvalid Variable Names\nReason\n\n\n\n\n2var\nStarts with a number\n\n\n_varname\nStarts with underscore\n\n\n.2var_name\nWhile starting with a (.) dot is valid, it can not be followed by a number\n\n\n\nNow that we have an idea of how to name variables, lets discuss variable assignments. Variables can be assigned values using leftward (&lt;-), rightward (-&gt;) and equal (=) operators. However, we will only stick with the leftward and equal assignment operators.\n\nvar_name1 &lt;- 10\nvar2 = 20\nvar.name3 &lt;- 30\nvar_name_4 = 40"
  },
  {
    "objectID": "notes/01-fundamentals.html#variables",
    "href": "notes/01-fundamentals.html#variables",
    "title": "Fundamentals of R",
    "section": "",
    "text": "A variable provides us with named objects that our programs can manipulate. A valid variable name consists of letters, numbers and the dot or underline characters. It is important to note variable names are case sensitive. That is, var1 and Var1 are different variables. Below are appropriate variable names in R\n\n\n\n\n\n\n\nValid Variable Name\nReason\n\n\n\n\nvariable_name\nContains letters and underscore\n\n\nlong.variable_name\nContains letters, dot, and underscore\n\n\nvar\nContains letters\n\n\nvar1\nContains letters and numbers\n\n\nlong.variable_name2\nContains letters, numbers, dot and underscore\n\n\nvar1_name.1\nContains letters, numbers, dot and underscore\n\n\n.var_name\nCan start with period, contains letters and underscore\n\n\n\nThis is a good starting point for valid variable names. Next we demonstrate a few examples where variable names are invalid.\n\n\n\n\n\n\n\nInvalid Variable Names\nReason\n\n\n\n\n2var\nStarts with a number\n\n\n_varname\nStarts with underscore\n\n\n.2var_name\nWhile starting with a (.) dot is valid, it can not be followed by a number\n\n\n\nNow that we have an idea of how to name variables, lets discuss variable assignments. Variables can be assigned values using leftward (&lt;-), rightward (-&gt;) and equal (=) operators. However, we will only stick with the leftward and equal assignment operators.\n\nvar_name1 &lt;- 10\nvar2 = 20\nvar.name3 &lt;- 30\nvar_name_4 = 40"
  },
  {
    "objectID": "notes/01-fundamentals.html#vectors",
    "href": "notes/01-fundamentals.html#vectors",
    "title": "Fundamentals of R",
    "section": "Vectors",
    "text": "Vectors\nThe easiest method to create any type of vector in R is using c() (as in concatenate). We primarily focus on two types of vectors; numeric and character\n\nNumeric vectors\n\nnum_vec &lt;- c(0,1,2,3,4)\ntypeof(num_vec)\n\n[1] \"double\"\n\nclass(num_vec)\n\n[1] \"numeric\"\n\n\nUsing c() is not the only way to generate a vector, we can also generate the above vector using seq() as follows\n\nseq(from=0,to=4)\n\n[1] 0 1 2 3 4\n\n\nAnother approach to generate the same sequence can be done using 0:4\n\n0:4\n\n[1] 0 1 2 3 4\n\n\nor more generally a:b, where a is the starting number and b is the last number in the sequence\nWe can apply arithmetic operations to our numerical vector num_vec, such as addition, subtraction, multiplication, division, and exponentiation. These operations will be applied to each element in the vector (element-wise).\n\n\n\nOperator\nDescription\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^\nExponent\n\n\n%%\nModulus (Remainder from division)\n\n\n%/%\nInteger Division\n\n\n\nArithmetic operations applied to numeric vectors follow PEMDAS order of operations, demonstrated in the following example\nSubtract 1 from each element\n\n(num_vec-1)\n\n[1] -1  0  1  2  3\n\n\nSubtract 1 from each element, then square them\n\n(num_vec-1)^2\n\n[1] 1 0 1 4 9\n\n\nSubtract 1 from each element, square them, then double each element\n\n 2*(num_vec - 1)^2\n\n[1]  2  0  2  8 18\n\n\nSubtract 1 from each element, square them, double them, then add 1 to each element\n\n2*(num_vec - 1)^2 + 1\n\n[1]  3  1  3  9 19\n\n\n\npemdas_vec &lt;- 2*(num_vec - 1)^2 + 1\npemdas_vec\n\n[1]  3  1  3  9 19\n\n\nGenerating an odd sequence from 1 to 9, we can use c() or seq()\n\nc(1,3,5,7,9)\n\n[1] 1 3 5 7 9\n\nseq(from =1,to=10,by=2)\n\n[1] 1 3 5 7 9\n\n\nNote if you know the ordering of the arguments of a function it is not necessary to specify them. For example, it is optional to write from and to arguments in the seq() function\n\nseq(from = 1,to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(1,10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\nCharacter vectors\n\nchr_vec &lt;- c('A','B',\"C\")\ntypeof(chr_vec)\n\n[1] \"character\"\n\nclass(chr_vec)\n\n[1] \"character\""
  },
  {
    "objectID": "notes/01-fundamentals.html#manipulating-vectors",
    "href": "notes/01-fundamentals.html#manipulating-vectors",
    "title": "Fundamentals of R",
    "section": "Manipulating vectors",
    "text": "Manipulating vectors\nThere are multiple ways to access or replace values in vectors. The most common approach is through “indexing”. It is important to know in starts with index 1.\n\nbig_vec &lt;- 1:100\nbig_vec[1]\n\n[1] 1\n\nbig_vec[10] # extract the 10th element in your vector\n\n[1] 10\n\n\nFor accessing elements in a vector we can think vector[indices you want to extract] the way we extract certain elements can be through some condition, that is vector[condtion]\n\nbig_vec[ c(1,5,10) ]\n\n[1]  1  5 10\n\nbig_vec[ 1:10 ] # what are the first 10 elements ?\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nUsing c() we can concatenate elements from one vector into another vector. For example, we can add the elements from pemdas_vec into the existing vector num_vec\n\nc(num_vec,pemdas_vec)\n\n [1]  0  1  2  3  4  3  1  3  9 19\n\n\nAlternatively, we can add the elements from num_vec into the existing vector pemdas_vec\n\nc(pemdas_vec,num_vec)\n\n [1]  3  1  3  9 19  0  1  2  3  4\n\n\nYou will notice the order in which we concatenate the elements from the vectors does matter\n\nchr_vec\n\n[1] \"A\" \"B\" \"C\"\n\nchr_vec[1] &lt;- 'a'\nchr_vec\n\n[1] \"a\" \"B\" \"C\"\n\n\n\nnum_vec\n\n[1] 0 1 2 3 4\n\nnum_vec[3] &lt;- 10\nnum_vec\n\n[1]  0  1 10  3  4\n\nnum_vec[ c(1,3) ] &lt;- c(100,200)\nnum_vec\n\n[1] 100   1 200   3   4\n\nnum_vec[c(1,2,3)] &lt;- 0\nnum_vec\n\n[1] 0 0 0 3 4"
  },
  {
    "objectID": "notes/01-fundamentals.html#installing-packages",
    "href": "notes/01-fundamentals.html#installing-packages",
    "title": "Fundamentals of R",
    "section": "Installing packages",
    "text": "Installing packages\nWhile base R contains a wide collection of useful functions and datasets, it might be necessary to install additional R packages to increase the power of R by improving existing base R functionalities, or by adding new ones.\nIn general, you can use this template to install a package in R:\n\ninstall.packages('package_name')\n\nFor example, in this lab we will need functions/datasets from the following package: maps. To install we simply type in our console\n\ninstall.packages('maps')\n\nAfter running the above command you should get something similar to the output below. The messages appeared will depend on what operating system you are using, the dependencies, and if the package was successfully installed.\n\ntrying URL 'https://cran.rstudio.com/bin/macosx/contrib/4.2/maps_3.4.0.tgz'\nContent type 'application/x-gzip' length 3105764 bytes (3.0 MB)\n==================================================\ndownloaded 3.0 MB\n\n\nThe downloaded binary packages are in\n    /var/folders/mc/rznpg9ks30sd6wdh7rchs4v40000gn/T//RtmpLUHvkq/downloaded_packages\n\nOnce the package was installed successfully we now have access to all of its functionalities/datasets. To access them we load the package into memory using the command library()\n\nlibrary(maps)\n\nHowever, if we only need to access say a specific function/dataset a few times we can do so using the notation packagename::functionname(). For example, if we only need to access the Canada cities data set in the maps package we run the following command\n\nmaps::canada.cities\n\n\n\n           name country.etc    pop   lat    long capital\n1 Abbotsford BC          BC 157795 49.06 -122.30       0\n2      Acton ON          ON   8308 43.63  -80.03       0\n3 Acton Vale QC          QC   5153 45.63  -72.57       0\n4    Airdrie AB          AB  25863 51.30 -114.02       0\n5    Aklavik NT          NT    643 68.22 -135.00       0\n\n\nAlternatively, if you loaded the entire package using library(maps) we can access the Canada cities data set using the following command\n\ncanada.cities\n\n\n\n           name country.etc    pop   lat    long capital\n1 Abbotsford BC          BC 157795 49.06 -122.30       0\n2      Acton ON          ON   8308 43.63  -80.03       0\n3 Acton Vale QC          QC   5153 45.63  -72.57       0\n4    Airdrie AB          AB  25863 51.30 -114.02       0\n5    Aklavik NT          NT    643 68.22 -135.00       0"
  },
  {
    "objectID": "notes/11-confidence-interval.html",
    "href": "notes/11-confidence-interval.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Required Packages\n\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(openintro)\n\n\ntheme_set(theme_bw())\ntheme_replace(panel.grid.minor = element_blank(),\n              panel.grid.major = element_blank())\nConfidence intervals provide a range of values that indicates the level of uncertainty associated with an estimate. This helps us understand the precision of the estimate, as apposed to a point estimate.\nFrom the plot above, we see that 95 of the 100 confidence intervals cover the population parameter \\(\\mu = 0\\). While it’s important to note that if we were to repeat the simulation another 100 times, the precise count may vary, but it is highly probable that it will remain close to 95\nIn the below plots, we repeat the same process mentioned above but this time constructing intervals from 100, 500, and 1000 samples each of size 25. The coverage percentage is demonstrated in the title of each respective plot"
  },
  {
    "objectID": "notes/11-confidence-interval.html#inference-for-a-population-mean",
    "href": "notes/11-confidence-interval.html#inference-for-a-population-mean",
    "title": "Confidence Intervals",
    "section": "Inference for a Population Mean",
    "text": "Inference for a Population Mean\nWe consider the Starbucks data set from the package openintro. This data gives nutrition facts for several food items at Starbucks, we are primarily interested in the average calories in the their food items.\n\nstarbucks &lt;- openintro::starbucks\n\n\n\n#&gt; # A tibble: 6 × 7\n#&gt;   item                        calories   fat  carb fiber protein type  \n#&gt;   &lt;chr&gt;                          &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;fct&gt; \n#&gt; 1 8-Grain Roll                     350     8    67     5      10 bakery\n#&gt; 2 Apple Bran Muffin                350     9    64     7       6 bakery\n#&gt; 3 Apple Fritter                    420    20    59     0       5 bakery\n#&gt; 4 Banana Nut Loaf                  490    19    75     4       7 bakery\n#&gt; 5 Birthday Cake Mini Doughnut      130     6    17     0       0 bakery\n#&gt; 6 Blueberry Oat Bar                370    14    47     5       6 bakery\n\n\nWe create the sampling distribution for the sample proportion of tenured professors and compare it to the population distribution of all the professors ranks\n\nn_samples &lt;- 10000\nsample_size &lt;- 30\ncalories &lt;- starbucks$calories\nsample_calories &lt;- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  sample(calories, size = sample_size) # generate a new sample from the population\n  sample_calories[i] = mean(sample_i) # obtain proportion for each sample\n}\n\n\n\nShow Code\n\n\npop_plt &lt;- ggplot(starbucks) +\n geom_histogram(mapping = aes(x = calories), bins = 30,\n          fill='steelblue',alpha = 0.3,color='black')+\n  labs(title = 'Population Distribution',\n       subtitle= \"Calories of Food\",\n       y = 'Counts', x = 'Calories')\n\n\nsampling_dist &lt;- ggplot(data.frame(sample_calories),\n                        aes(sample_calories))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,bins = 30,\n                 color = 'black')+\n  labs(title = 'Sampling distribution of Sample Mean',\n       subtitle = \"Calories of Food\",\n       x = 'Calories',y = '')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nSampling\n\n\n\n\nShape\nskewed\nnormal/bell-shaped\n\n\nMean\n338.8311688\n338.6314\n\n\nSD\n\\(\\sigma =\\) 105.3687014\n\\(\\frac{\\sigma}{\\sqrt{n}} =\\) 19.2376049\n\n\n\nWhen the sampling distribution is roughly normal in shape, then we can construct an interval that expresses exactly how much sampling variability there is. Using our single sample of data and the properties of the normal distribution, we can be 95% confident that the population parameter is within the following interval \\[\n\\left[\\overline{x} - ME \\, \\, ,  \\, \\,\\overline{x} + 1.96 ME \\right]\n\\]\nwhere the margin of error \\(ME = \\text{critical value} \\times SE\\). The critical value for a \\(100(1-\\alpha\\))% CI can be obtained by qnorm(p = 1-alpha/2) whenever the sample size is large enough, say \\(n=30\\) and the sampling distribution is approximately normal (bell-shaped)\nFor example, a 90% = 100(1-0.1)% CI can be calculated as\n\nalpha = 0.1\nqnorm(p = 1-alpha/2)\n\n#&gt; [1] 1.644854\n\n\nCommonly used critical values are\n\n\n\nConfidence Level\nCritical Value\nR code\n\n\n\n\n99%\n2.58\nqnorm(p = 1-0.01/2)\n\n\n95%\n1.96\nqnorm(p = 1-0.05/2)\n\n\n90%\n1.65\nqnorm(p = 1-0.1/2)\n\n\n\nThe standard error can be approximated using \\(SE = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}}\\), where \\(s\\) is the standard deviation of the sample obtained from the population\nPutting all of this together, a 95% CI is \\[\n\\left[\\overline{x} - 1.96 \\frac{s}{\\sqrt{n}} \\, \\, , \\, \\, \\overline{x} + 1.96 \\frac{s}{\\sqrt{n}} \\right]\n\\]\n\nset.seed(1)\nsample_calories &lt;- sample(calories, 30)\nxbar_calories &lt;- mean(sample_calories)\nsd_calories &lt;- sd(sample_calories)\n\nFor our Starbucks calories example, A sample mean obtain from a SRS is \\(\\overline{x} =\\) 353.333 and standard deviation \\(s=\\) 90.984 then the 95% CI is\n\nlower_bound &lt;- xbar_calories - 1.96*(sd_calories/sqrt(30))\nupper_bound &lt;- xbar_calories + 1.96*(sd_calories/sqrt(30))\n\n\nc(lower_bound, upper_bound)\n\n#&gt; [1] 320.7750 385.8917\n\n\n\nWe are 95% confident the population avarage for calories of food at Starbucks is between 320.775 and 385.892"
  },
  {
    "objectID": "notes/11-confidence-interval.html#inference-for-a-population-proportion",
    "href": "notes/11-confidence-interval.html#inference-for-a-population-proportion",
    "title": "Confidence Intervals",
    "section": "Inference for a Population Proportion",
    "text": "Inference for a Population Proportion\nWe consider the Professor evaluations and beauty data from the package openintro. This data was gathered from end of semester student evaluations for 463 courses taught by a sample of 94 professors from the University of Texas at Austin. In addition, six students rate the professors’ physical appearance. The result is a data frame where each row contains a different course and each column has information on the course and the professor who taught that course\n\nprofessor_evaluations &lt;- openintro::evals\n\n\n\n#&gt; # A tibble: 6 × 23\n#&gt;   course_id prof_id score rank     ethnicity gender language   age cls_perc_eval\n#&gt;       &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;     &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;         &lt;dbl&gt;\n#&gt; 1         1       1   4.7 tenure … minority  female english     36          55.8\n#&gt; 2         2       1   4.1 tenure … minority  female english     36          68.8\n#&gt; 3         3       1   3.9 tenure … minority  female english     36          60.8\n#&gt; 4         4       1   4.8 tenure … minority  female english     36          62.6\n#&gt; 5         5       2   4.6 tenured  not mino… male   english     59          85  \n#&gt; 6         6       2   4.3 tenured  not mino… male   english     59          87.5\n#&gt; # ℹ 14 more variables: cls_did_eval &lt;int&gt;, cls_students &lt;int&gt;, cls_level &lt;fct&gt;,\n#&gt; #   cls_profs &lt;fct&gt;, cls_credits &lt;fct&gt;, bty_f1lower &lt;int&gt;, bty_f1upper &lt;int&gt;,\n#&gt; #   bty_f2upper &lt;int&gt;, bty_m1lower &lt;int&gt;, bty_m1upper &lt;int&gt;, bty_m2upper &lt;int&gt;,\n#&gt; #   bty_avg &lt;dbl&gt;, pic_outfit &lt;fct&gt;, pic_color &lt;fct&gt;\n\n\nWe are interested in the proportion of professors who are of rank “Tenured”. The proportions of the professors ranks are shown below\n\ntable(professor_evaluations$rank) |&gt; \n  prop.table()\n\n#&gt; \n#&gt;     teaching tenure track      tenured \n#&gt;    0.2203024    0.2332613    0.5464363\n\n\nWe create the sampling distribution for the sample proportion of tenured professors and compare it to the population distribution of all the professors ranks\n\nn_samples &lt;- 10000\nsample_size &lt;- 50\nrank_proportions &lt;- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  sample(professor_evaluations$rank, size = sample_size) # generate a new sample from the population\n  rank_proportions[i] = mean(sample_i == 'tenured') # obtain proportion for each sample\n}\n\n\n\nShow Code\n\n\nsampling_dist &lt;- ggplot(data.frame(rank_proportions),\n                        aes(rank_proportions))+\n  geom_bar(fill = 'steelblue',alpha = 0.3,\n                 color = 'black')+\n  labs(title = 'Sampling distribution of Sample Proportion',\n       subtitle = \"Professor Ranks\",\n       x = 'Proportion of Tenured',y = '')\n\n\npop_plt &lt;- ggplot(professor_evaluations) +\n geom_bar(mapping = aes(x = rank, y = ..prop.., group = 1), stat = \"count\",\n          fill='steelblue',alpha = 0.3,color='black')+\n  labs(title = 'Population Distribution',\n       subtitle= \"Professor Ranks\",\n       y = 'Proportion', x = 'Rank')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation\nSampling\n\n\n\n\nShape\n\nnormal/bell-shaped\n\n\nMean\n\\(p=\\) 0.5464363\n\\(\\hat{p}=\\) 0.546252\n\n\nSD\n\\(\\sigma = \\sqrt{p(1-p)} =\\) 0.497839\n\\(\\sqrt{\\frac{p(1-p)}{n}} =\\) 0.0704075\n\n\n\nWe can form a 95% confidence interval for the population proportion of professors who are tenured rank at the University of Texas at Austin \\[\n\\left( \\hat{p} - \\, 1.96 \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\, \\, ,  \\quad\n\\hat{p}+ \\, 1.96 \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right)\n\\]\nGiving us the following 95% CI (0.408,0.684). We can see the constructed interval contains the population proportion of 0.5464363. A simple interpretation of this confidence interval is\n\nWe are 95% confident that the population proportion of tenured professors at the University of Texas is between 0.408 and 0.684"
  },
  {
    "objectID": "notes/09-normal_distribution.html",
    "href": "notes/09-normal_distribution.html",
    "title": "Normal Distribution",
    "section": "",
    "text": "Required Packages\n\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\ntheme_set(theme_bw())\ntheme_replace(panel.grid.minor = element_blank(),\n              panel.grid.major = element_blank())"
  },
  {
    "objectID": "notes/09-normal_distribution.html#normal-distribution",
    "href": "notes/09-normal_distribution.html#normal-distribution",
    "title": "Normal Distribution",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nOne of the most important and widely used continuous distribution is the Normal distribution, or Gaussian distribution.\nLet \\(X \\sim N(\\mu,\\sigma)\\) be a random variable following a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). In R, the following functions described in the table below, allows us to summarize the function relating to the normal distribution\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ndnorm\nNormal density\n(Probability Density Function)\n\n\npnorm\nNormal distribution\n(Cumulative Distribution Function)\n\n\nqnorm\nQuantile function of the Normal distribution\n\n\nrnorm\nNormal random number generation\n\n\n\nBy default, all of the functions above consider the standard Normal distribution, which has a mean of zero and a standard deviation of one, \\(X \\sim N(0,1)\\)\n\ndnorm\nThe density function for a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp{\\left(\n-\\frac{1}{2\\sigma^2 }(x-\\mu)^2\n\\right)}\n\\] for \\(-\\infty &lt; x &lt; \\infty\\)\nWe can use dnorm() function to calculate the density function, i.e \\(f(x)\\), for a grid of \\(x\\) values from any normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)\nFor example, we can calculate \\(f(0)\\) from a standard normal distribution\n\ndnorm(x = 0,mean = 0,sd = 1)\n\n#&gt; [1] 0.3989423\n\n\nConsider evaluating \\(f(x)\\) for \\(x \\in [1,10]\\) with mean 1 and standard deviation of 3\n\ndnorm(x=1:10,mean = 1,sd = 3)\n\n#&gt;  [1] 0.132980760 0.125794409 0.106482669 0.080656908 0.054670025 0.033159046\n#&gt;  [7] 0.017996989 0.008740630 0.003798662 0.001477283\n\n\n\n\npnorm\nThe pnorm() function gives the Cumulative Distribution Function (CDF) of the Normal distribution, which is the probability that the variable \\(X\\) takes a value less than or equal to \\(x\\). Mathematically, \\(F_X(x) = P(X \\leq x)\\).\nFor any continuous distribution \\(P(X = x)=0\\), so equivalently the CDF is \\(P(X \\leq x) = P(X &lt; x)\\).\nConsider the standard normal distribution, since this distribution is symmetrical centered around \\(\\mu=0\\) then \\(P(X \\leq 0) = 0.5\\). We can verify this result using pnorm as follows\n\npnorm(0,mean = 0, sd = 1)\n\n#&gt; [1] 0.5\n\n\nExample: Suppose \\(X\\) is the SAT-M score which has a normal distribution with a mean of 507 and standard deviation of 111. What is the probability of scoring less than 700 on the SAT-M?\n\n\nShow Code\n\n\nprob1 &lt;- round(pnorm(700, mean=507, sd=111) * 100,2)\nplt1 &lt;- ggplot(data.frame(x = c(150,900)), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = c(150,900),\n                args = list(mean = 507,sd = 111)) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = 'steelblue',\n                alpha =0.3,\n                xlim = c(150, 700),\n                args = list(mean = 507,sd = 111))+\n  annotate(\"text\", x = 510, y = 0.0015, \n           label = paste0(prob1,'%'),\n           size = 8)+\n  geom_vline(xintercept = 700,linetype =2)+\n  labs(x = '',y= 'Density')\n\n\n\n\n\n\n\nThat is \\(P(X &lt; 700)\\),\n\npnorm(700, mean=507, sd=111)\n\n#&gt; [1] 0.9589596\n\n\nWhat about the probability of scoring greater than 700?\n\n\nShow Code\n\n\nprob2 &lt;- round(pnorm(700, mean=507, sd=111,\n                     lower.tail = FALSE)*100,2)\n\np2 &lt;- ggplot(data.frame(x = c(150,900)), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = c(150,900),\n                args = list(mean = 507,sd = 111)) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = 'steelblue',\n                alpha =0.3,\n                xlim = c(700, 900),\n                args = list(mean = 507,sd = 111))+\n  annotate(\"text\", x = 738, y = 0.00017, \n           label = paste0(prob2,'%'),\n           size=4.7)+\n  geom_vline(xintercept = 700,linetype =2)+\n  labs(x = '',y= 'Density')\n\n\n\n\n\n\n\nWe are interested \\(P(X &gt; 700)\\), which can be obtained through \\(P(X &gt; 700) = 1- P(X \\leq 700)\\)\n\n1-pnorm(700, mean = 507, sd = 111)\n\n#&gt; [1] 0.04104036\n\n\nAlternative, pnorm() has an argument lower.tail=TRUE (by default). If lower.tail=TRUE, the probabilities \\(P(X \\leq x)\\) are returned. Otherwise, if lower.tail=FALSE, \\(P(X &gt; x)\\) are returned\n\npnorm(700, mean = 507, sd = 111, lower.tail = FALSE)\n\n#&gt; [1] 0.04104036\n\n\nThe Empirical rule (also known as the 68-95-99.7 rule) is a statistical rule stating that for a normal distribution, where most of the data will fall within three standard deviations of the mean. The empirical rule can be broken down into three parts: 68% of data falls within the first standard deviation from the mean (blue shaded region). 95% fall within the 2nd standard deviations (up to the green shaded region). 99.7% fall within third standard deviation (up to the red shaded region)\n\n\nShow Code\n\n\nx_limits &lt;- c(-4,4)\n\n\nplot_shaded_normal &lt;- function(shade_limits,percent_display,\n                               annotate_x_label,annotate_y_label,\n                               x_limits = c(-4,4),\n                               fill = 'steelblue',\n                               alpha = 0.3,annotate_label_size=4.5){\n  plt &lt;- ggplot(data.frame(x = x_limits), aes(x)) +\n    stat_function(fun = dnorm,geom = \"line\",xlim = x_limits) +\n    stat_function(fun = dnorm,geom = \"area\",fill = fill,alpha =alpha,\n                  xlim = shade_limits)+\n    labs(x = '',y= 'Density')+\n    annotate(\"text\", x = annotate_x_label, y = annotate_y_label, \n             label = paste0(percent_display,'%'),\n             size=annotate_label_size)+\n    scale_x_continuous(name = '',limits = x_limits,\n                       breaks = x_limits[1]:x_limits[2])\n  return(plt)\n}\n\n\nplots &lt;- plot_shaded_normal(shade_limits = c(-1,1),\n                            percent_display = 68,\n                            annotate_x_label = 0.1,annotate_y_label = 0.15)+\n  plot_shaded_normal(shade_limits = c(-2, 2),fill='green',\n                     percent_display = 95,\n                     annotate_x_label = 0,annotate_y_label = 0.15)+\n  plot_shaded_normal(shade_limits = c(-3, 3),fill='red',\n                     percent_display = 99.7,\n                     annotate_x_label = 0.1,annotate_y_label = 0.15)+\n  plot_annotation(title = '68–95–99.7 Rule')\n\n\n\n\n\n\n\nWe can easily verify these results using pnorm. Assuming a standard Normal distribution if we were one standard deviation away from the mean then\n\npnorm(1)-pnorm(-1)\n\n#&gt; [1] 0.6826895\n\n\nIf we were two standard deviations away from the mean\n\npnorm(2) - pnorm(-2)\n\n#&gt; [1] 0.9544997\n\n\nand lastly, three standard deviations away from the mean\n\npnorm(3) - pnorm(-3)\n\n#&gt; [1] 0.9973002\n\n\n\n\nqnorm\nThe function qnorm() returns the value of the inverse cumulative density function (CDF) of the normal distribution with specified mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nLet \\(F_X(x) = P(X \\leq x)\\) be the CDF of the normal distribution, and suppose it returns the probability \\(p\\), i.e, \\(F_X(x) = p\\). The inverse of the CDF or (quantile function) tells you what \\(x\\) would make \\(F_X(x)\\) return some probability \\(p\\);\n\\[F_X^{-1}(p) = x\\] For example, for the standard normal distribution \\(F_X(0)=P(X \\leq 0) = 0.5\\). That is, the value of \\(x\\) or (quantile) which gives us a cumulative probability of 0.5 is \\(x=0\\).\nTherefore, we can use the qnorm() function to find out what value of \\(x\\) or (quantile) gives us a a cumulative probability of \\(p\\). Hence, the qnorm function is the inverse of the pnorm function\n\n\nShow Code\n\n\nx_limits &lt;- c(-4,4)\n\np1 &lt;- ggplot(data.frame(x = x_limits), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = x_limits) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = 'steelblue',\n                alpha =0.3,\n                xlim = c(-4, 0))+\n  geom_vline(xintercept = 0,linetype=2,color = '#cf5d55')+\n  labs(x = '',y= 'Density')+\n  annotate('text', x = -0.65, y = 0.17, \n           parse =TRUE,\n           label = paste0(\n             expression('F'[x]),'(0)==0.5') )+\n  annotate('text',x = 0.77, y = 0.41,\n           parse =TRUE,\n           label = paste0(\n             expression('F'[x]^{-1}),' *(0.5)==0'),\n           color = '#cf5d55')+\n  scale_x_continuous(name = '',limits = x_limits,\n                     breaks = -4:4)\n\n\n\n\n\n\n\n\nqnorm(p=0.5)\n\n#&gt; [1] 0\n\n\nGoing back to our example of the SAT-M scores. Suppose \\(X\\) is the SAT-M score which has a normal distribution with a mean of 507 and standard deviation of 111. Recall the probability of obtaining a score less than 700 on the SAT-M was \\(P(X &lt; 700) = 0.9589596\\). Therefore, if we were interesting in finding the score or (quantile) which gives us a cumulative probability of roughly 96% we can use qnorm as follows:\n\nqnorm(0.9589596, mean=507, sd=111)\n\n#&gt; [1] 700\n\n\nWe should see that the output value is exactly 700.\n\n\nrnorm\nThe rnorm function generates \\(n\\) observations from a Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nWe specify a seed for reproducibility,\n\nset.seed(10)\n\nLet’s start by generate 10 random observations from a standard normal distribution\n\nrnorm(10, mean = 0, sd = 1)\n\n#&gt;  [1]  0.01874617 -0.18425254 -1.37133055 -0.59916772  0.29454513  0.38979430\n#&gt;  [7] -1.20807618 -0.36367602 -1.62667268 -0.25647839\n\n\nor equivalently,\n\nrnorm(10)\n\n#&gt;  [1]  1.10177950  0.75578151 -0.23823356  0.98744470  0.74139013  0.08934727\n#&gt;  [7] -0.95494386 -0.19515038  0.92552126  0.48297852\n\n\nWe can specify a different mean and standard deviation\n\nrnorm(10, mean = 10, sd = 2)\n\n#&gt;  [1] 8.807379 5.629426 8.650268 5.761878 7.469604 9.252677 8.624889 8.255682\n#&gt;  [9] 9.796478 9.492439\n\n\nIn the following plot we generate \\(n=100,1000,10000\\) random observations from a standard normal distribution. If we increase the number of observations, we see the data will approach the true Normal density function\n\n\nShow Code\n\n\nplot_random_normal &lt;- function(n_samples, mean = 0, sd = 1,\n                               fill = 'steelblue', alpha = 0.3,\n                               bins = 30, density_linewidth = 1.2,\n                               density_linetype = 2){\n  \n  plt &lt;- ggplot(data = data.frame(x=rnorm(n_samples,mean = mean, sd = sd),\n                                  n_samples),aes(x))+\n    geom_histogram(aes(y = after_stat(density)),\n                   colour = 1, fill = fill,\n                   alpha = alpha,\n                   bins=bins) +\n    geom_density(linewidth = density_linewidth,\n                 linetype = density_linetype,\n                 colour = 2)+\n    labs(x= '',y = 'density')+\n    facet_grid(~n_samples)\n  return(plt)\n}\n\n\nplots &lt;- plot_random_normal(n_samples = 100)+\n  plot_random_normal(n_samples = 1000)+\n  plot_random_normal(n_samples = 10000)+\n  plot_layout(ncol=3)"
  },
  {
    "objectID": "notes/09-normal_distribution.html#standardizing-values",
    "href": "notes/09-normal_distribution.html#standardizing-values",
    "title": "Normal Distribution",
    "section": "Standardizing Values",
    "text": "Standardizing Values\nThe normal model \\(N(0,1)\\) is called the standard normal distribution, and it is the default model used when working with (.)norm functions\nA \\(z\\)-score, also known as a standard score, quantifies the number of standard deviations a data point is from the mean of a distribution. This can be useful for comparing and analyzing data across different scales or distributions, providing a standardized way to assess the relative position of a particular value within a dataset\nFor example, consider a scenario where you are comparing the performance of students in two different subjects, one with scores ranging from 0 to 100 and another with scores ranging from 0 to 50. Without standardization, it can be challenging to assess which student has performed relatively better across both subjects.\nThe \\(z\\)-score of a particular value is defined as \\[\nz = \\frac{\\text{value} - \\text{mean}}{\\text{standard deviation}}\n\\] If the values in their original units of measurement follow a normal distribution, then the \\(z\\)-scores will follow a standard normal distribution\nRecall the SAT-M scores example: Suppose \\(X\\) is the SAT-M score which has a normal distribution with a mean of 507 and standard deviation of 111. What is the probability of scoring less than 700 on the SAT-M?\n\npnorm(700, mean = 507, sd = 111)\n\n#&gt; [1] 0.9589596\n\n\nWe can also compute this quantity with the \\(z\\)-score with respect to the standard normal distribution \\[\n\\begin{align*}\nz &= \\frac{\\text{value} - \\text{mean}}{\\text{standard deviation}} \\\\[10pt]\nz &= \\frac{700 - 507}{111} \\\\[10pt]\nz &= 1.738739\n\\end{align*}\n\\]\n\nz &lt;- (700-507)/111\npnorm(z)\n\n#&gt; [1] 0.9589596\n\n\n\n\nShow Code\n\n\nprob2 &lt;- round(z * 100,2)\np2 &lt;- ggplot(data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = c(-4,4),\n                args = list(mean = 0,sd = 1)) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = 'red',\n                alpha =0.3,\n                xlim = c(-4, z),\n                args = list(mean = 0,sd = 1))+\n  annotate(\"text\", x = (510-507)/111, y = 0.15, \n           label = paste0(prob1,'%'),\n           size = 8)+\n  geom_vline(xintercept = z,linetype =2)+\n  labs(x = '',y= 'Density')\n\n\n\n\n\n\n\nBy standardizing the scores, we shift from calculating \\(P(X &lt; 700)\\) where the units are in SAT-M scores to equivalently computing \\(P(Z &lt; 1.738739)\\). This represents the number of standard deviations a data point is from the mean of the distribution. By standardizing we can now compare these results with say scores obtained from an english SAT exam on the same scale"
  },
  {
    "objectID": "notes/04-intro_categorical_variables.html",
    "href": "notes/04-intro_categorical_variables.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "We will use the births data set to summarize and visualize categorical variables using the base R approach\nCategorical data is a type of data that is divided into categories or groups, such as hair color or education level. Categorical data can be further divided into nominal data, which is data that has no inherent order, such as hair color, and ordinal data, which is data that has a defined order, such as education level"
  },
  {
    "objectID": "notes/04-intro_categorical_variables.html#single-categorical-variable",
    "href": "notes/04-intro_categorical_variables.html#single-categorical-variable",
    "title": "Summarizing Categorical Data",
    "section": "Single categorical variable",
    "text": "Single categorical variable\nOne way to extract all the character columns is using both Filter and is.character functions. A similar argument can be said for any columns that are factors, but instead we’d use is.factor.\nBelow are the first five rows of all the character columns in birth_dat\n\nFilter(is.character,birth_dat)\n\n\n\n  Gender Premie   Marital Racemom Racedad   Hispmom   Hispdad     Habit\n1   Male     No   Married   White   White   Mexican   Mexican NonSmoker\n2   Male     No Unmarried   White Unknown   NotHisp   Unknown    Smoker\n3   Male     No   Married   White   White OtherHisp OtherHisp NonSmoker\n4   Male     No   Married   White   White   Mexican   Mexican NonSmoker\n5 Female     No Unmarried   Black Unknown   NotHisp   Unknown NonSmoker\n  MomPriorCond BirthDef    DelivComp BirthComp\n1         None     None         None      None\n2 At Least One     None         None      None\n3         None     None At Least One      None\n4         None     None At Least One      None\n5         None     None         None      None\n\n\nThe names of the character columns can be obtained using colnames() function in combination with the above statement\n\ncolnames(Filter(is.character,birth_dat) )\n\n [1] \"Gender\"       \"Premie\"       \"Marital\"      \"Racemom\"      \"Racedad\"     \n [6] \"Hispmom\"      \"Hispdad\"      \"Habit\"        \"MomPriorCond\" \"BirthDef\"    \n[11] \"DelivComp\"    \"BirthComp\"   \n\n\nWe will only consider the Hispmom variable from our dataset to demonstrate methods to summarize and visualize a character variable.\nFirst, we’ll save the values from Hispmom column into a separate variable and compute several categorical summaries\n\nhispanic_mom &lt;- birth_dat$Hispmom\n\nThe table() function in R can be used to quickly create frequency tables.\n\ntable(hispanic_mom)\n\nhispanic_mom\n  Mexican   NotHisp OtherHisp \n      216      1697        85 \n\n\nFrom the above frequency table we observe there were 25 mom who were Mexican, 1693 non Hispanic, and 84 were other types of Hispanic. We can easily convert the frequency table into a frequency table of proportions using prop.table(). The input for prop.table() is a table created using table().\n\nprop.table(table(hispanic_mom))\n\nhispanic_mom\n   Mexican    NotHisp  OtherHisp \n0.10810811 0.84934935 0.04254254 \n\n\nNow, we observe roughly 10.79% of moms were Mexican, 84.99% were non Hispanic and 4.22% were other types of Hispanic. Note that all of the proportions should add up to 1.\n\nsum(prop.table(table(hispanic_mom)))\n\n[1] 1\n\n\nWhile the above method works, it is not the only way to obtain frequency tables. We can obtain the same results using tally() from the mosaic library.\n\nmosaic::tally(hispanic_mom)\n\nX\n  Mexican   NotHisp OtherHisp \n      216      1697        85 \n\n\nIf we want frequency tables of proportions, we need to use the argument format and specify format = 'proportion'. There are other formats such as 'count', 'percent' etc.. for more details run ?mosaicCore::tally()\n\nmosaic::tally(hispanic_mom,format='proportion')\n\nX\n   Mexican    NotHisp  OtherHisp \n0.10810811 0.84934935 0.04254254 \n\n\nTo plot a single categorical variable we can use barplot(). The input for barplot() when dealing with categorical data is a table, like the ones we created above\n\nbarplot(table(hispanic_mom))\n\n\n\n\nInstead of the frequency counts, we can plot frequency of proportions by inputting a frequency tables of proportions.\n\nbarplot(prop.table(table(hispanic_mom)),\n        main = 'Ethnicity Proportions of Moms',\n        col = '#d59cdb')"
  },
  {
    "objectID": "notes/04-intro_categorical_variables.html#two-categorical-variables",
    "href": "notes/04-intro_categorical_variables.html#two-categorical-variables",
    "title": "Summarizing Categorical Data",
    "section": "Two categorical variables",
    "text": "Two categorical variables\nFor this example, we consider the following two character variables Hispdad and Habit. Hispdad determines whether the father of the baby was Hispanic or not. In particular, are they Mexican, non-Hispanic, or other type of Hispanic ethnicity. Habit determines whether or not the subject had a smoking habit or not.\nWhen dealing with two categorical variables we can create a two-way table using table(v1,v2). Below is the table of frequency for both Habit and Hispdad.\nNote: We save the table as a variable so we can use it later\n\nsmoker_hispanic_dad &lt;- table(birth_dat$Habit,birth_dat$Hispdad)\nsmoker_hispanic_dad\n\n           \n            Mexican NotHisp OtherHisp Unknown\n                  0       4         0       2\n  NonSmoker     184    1236        78     307\n  Smoker          5     117         2      63\n\n\nFrom the above frequency table of counts you will notice that there were 184 Mexican dads who were non-smokers, 5 Mexican dads who were smokers, 1236 non-Hispanics who were non-smokers, 117 non-Hispanics who were smokers and similar interpretations can be made for the remaining cells.\nWe can obtain a table of proportions using prop.table()\n\nprop.table(smoker_hispanic_dad)\n\n           \n                Mexican     NotHisp   OtherHisp     Unknown\n            0.000000000 0.002002002 0.000000000 0.001001001\n  NonSmoker 0.092092092 0.618618619 0.039039039 0.153653654\n  Smoker    0.002502503 0.058558559 0.001001001 0.031531532\n\n\nNow, lets plot the results of our table using the default barplot settings\n\nbarplot(smoker_hispanic_dad)\n\n\n\n\nIt is difficult to understand the meaning of the black and gray filled sections of the barplot. Although we may have a general understanding that the gray portion represents smokers and the black portion represents non-smokers based on the accompanying table, we should not assume that the reader will automatically make this connection.\nWe can add a legend by using the argument legend.text=TRUE, and barplot will use the row names of our table to make the legend. Moreover, we add appropriate labels to our plot\n\nbarplot(smoker_hispanic_dad,\n        legend.text = TRUE,\n        xlab = 'Ethnicity',\n        ylab = 'Counts')\n\n\n\n\nThe above figure shows a stacked bar plot. If we wanted the bars next to each other, rather than on top of each other, we can use the argument beside=TRUE.\n\nbarplot(smoker_hispanic_dad,\n        legend.text = TRUE,\n        beside = TRUE,\n        xlab = 'Ethnicity',\n        ylab = 'Counts')\n\n\n\n\nIt is evident that the number of non-smokers exceeds that of smokers across all ethnicities. However, we may be able to obtain a more comprehensive understanding of the data by altering the grouping order of the bars. Specifically, we should examine which ethnic group has a higher count for each smoking category.\nWe can change the order of our table by taking the transpose, that is we swap the columns and rows. In R, we can transpose any table-like object using the function t()\n\nt(smoker_hispanic_dad)\n\n           \n                 NonSmoker Smoker\n  Mexican      0       184      5\n  NotHisp      4      1236    117\n  OtherHisp    0        78      2\n  Unknown      2       307     63\n\n\nFrom this point of view, we can observe the number of counts in each smoking habit category for each ethnicity. For example, there were 184 Mexican fathers who are non-smokers and 5 Mexican fathers that did smoke. Similar, interpretations can be made for other ethnic groups.\nWe can now use barplot on this new transposed table\n\nbarplot(t(smoker_hispanic_dad),\n        legend.text = TRUE,\n        beside = TRUE,\n        xlab = 'Smoking Habit',\n        ylab = 'Counts')\n\n\n\n\nWe can clearly see the non-Hispanic fathers make up the highest counts for non-smokers and smokers. While the default color palette is color-blind friendly it can be hard to distinguish the categories based on these colors.\nWith a quick Google search of “four color palettes” you can find great palettes for 4 categories. For example, the following color palette was obtain from colorhunt.co\n\nbarplot(t(smoker_hispanic_dad),\n        legend.text = TRUE,\n        col =c('#4E6E81','#F9DBBB','#FF0303','#2E3840'),\n        beside = TRUE,\n        xlab = 'Smoking Habit',\n        ylab = 'Counts')"
  },
  {
    "objectID": "notes/07-for-loops.html",
    "href": "notes/07-for-loops.html",
    "title": "Introduction to For-Loops",
    "section": "",
    "text": "A for-loop serves the purpose of cycling through a collection of objects, such as a vector, list, matrix, or dataframe, and consistently applying a specific set of operations to each element within the data structure\nThe syntax of a for-loop in R consists of a variable which takes items from the iterable one by one, where the iterable is the collection of objects provided (vector, list, matrix, etc..)\nLastly, inside the for-loop within the curly braces { } is the loop body which are statements that are executed once for each item in the iterable provided\n\nfor(variable in iterable) {\n  loop body\n}\n\nUtilizing for-loops helps maintain code cleanliness and prevents unnecessary duplication of code blocks\nTo start with a basic example, consider printing the numbers from 1 to 5 inclusive, this is our iterable and is constructed using any sequence operator. This can be a:b or the built-in function seq()\n\nfor (index in 1:5){\n  print(index)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere index is our variable, the variable name can be anything but it is usually in the context of the problem\nFor example, this can be anything from car,letters, months, etc.., in most cases usually the variable name i suffices\nOur loop body is simply to print() the current index\n\nUsing a For-Loop on a vector\nIn the next example, we will print the numbers from 1 to 5 and then double each number before printing it\n\nfor(i in 1:5){\n  print(i*2)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n[1] 10\n\n\nHowever, there are scenarios where we not only want to print these values but also store them after each iteration. To do that, we need to start by creating an empty object. This object can be new or an existing one, but it’s important to keep in mind that after each iteration, the object may be modified\nThe function numeric() creates a numeric vector of all 0 of a specified length. Here the length, should be the the same size as the iterable\n\nvec &lt;- numeric(length = 5)\n\n\nvec\n\n[1] 0 0 0 0 0\n\n\n\nfor(i in 1:5){\n1  print(paste0('Current Iteration: ',i) )\n  \n2  vec[i] &lt;- (i*2)\n  \n3  print(vec)\n}\n\n\n1\n\nPrint the current iteration (optional)\n\n2\n\nUpdate the \\(i\\)th element of the vector vec by doubling the current index\n\n3\n\nPrint the updated vector vec (optional)\n\n\n\n\n[1] \"Current Iteration: 1\"\n[1] 2 0 0 0 0\n[1] \"Current Iteration: 2\"\n[1] 2 4 0 0 0\n[1] \"Current Iteration: 3\"\n[1] 2 4 6 0 0\n[1] \"Current Iteration: 4\"\n[1] 2 4 6 8 0\n[1] \"Current Iteration: 5\"\n[1]  2  4  6  8 10\n\n\nHere is the resulting vector\n\nvec\n\n[1]  2  4  6  8 10\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn R, for-loops tend to be slow. To mitigate the performance issues associated with for-loops, it is often recommended to use vectorized operations or apply functions\n\n\nFor example,\n\nvec &lt;- 2*(1:5)\nvec\n\n[1]  2  4  6  8 10\n\n\nIt is clear there was no need to use a for-loop in the previous example it was simply for teaching purposes\nWe don’t need to iterate sequentially from a regular sequence 1:N. We can iterate through the elements of an existing object. For example,\n\nfor(i in 3:6){\n  print(i)\n}\n\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n\n\n\nfor(i in seq(from = 1, to = 11, by = 2)){\n  print(i)\n}\n\n[1] 1\n[1] 3\n[1] 5\n[1] 7\n[1] 9\n[1] 11\n\n\n\nfor(pet in c('dog','cat','hamster','turtle')){\n  print(pet)\n}\n\n[1] \"dog\"\n[1] \"cat\"\n[1] \"hamster\"\n[1] \"turtle\"\n\n\nHowever, if you are trying to update the \\(i\\)th element of an object this approach might not be the best\n\nvec &lt;- numeric(length=3)\n\nfor(index in c(2,4,6) ){\n  print( vec[index] )\n}\n\n[1] 0\n[1] NA\n[1] NA\n\n\nAn NA is produced because our vector vec has three elements and we are trying to access the 4th and 6th element which do not exist\nInstead use the function seq_along(x) which will create a regular sequence from 1:length(x)\n\nseq_along(c(2,4,6))\n\n[1] 1 2 3\n\n\n\nseq_along(c('dog','cat','hamster','turtle'))\n\n[1] 1 2 3 4\n\n\n\nfor(index in seq_along(c(2,4,6)) ){\n  print(index)\n}\n\n[1] 1\n[1] 2\n[1] 3"
  },
  {
    "objectID": "notes/08-sampling_simulation.html",
    "href": "notes/08-sampling_simulation.html",
    "title": "Sampling and Simulation",
    "section": "",
    "text": "We can obtain a random sample from existing elements in a dataset, vector, or list using the sample() function in R\nThe basic syntax for the sample() function is as follows:\nsample(x,size,replace = FALSE, prob = NULL)\n\nx: object we are sampling from, most often a vector or dataset\nsize: size of the sample\nreplace: should sampling be with replacement? (FALSE by default)\nprob: A vector of probability weights for obtaining the elements of the vector being sampled (NULL by default, every element is equally likely to be drawn)\n\nFor more details on the sample function run ?sample in the console\nIn order to replicate the following examples and get similar results, we can use set.seed()\n\nset.seed(123)\n\nSuppose we have a vector with 10 elements in it\n\nvec &lt;- 1:10\n\nTo generate a random sample of five elements from our vector vec, we run the following command\n\nsample(vec,size = 5)\n\n#&gt; [1]  3 10  2  8  6\n\n\nRecall, the default setting for sampling is without replacement. This means once we draw the number (or element) from our vector we can not draw it again, similar to the lottery method.\nIt is also important to note that each time we run the command sample() it may generate a different set of elements each time\n\nsample(vec,size = 5)\n\n#&gt; [1] 5 4 6 8 1\n\n\nWe can also sample random elements from our vector using the argument replace=TRUE so that we are sampling with replacement. That is, each time we draw an element we put it back into our vector and there may be a chance we draw it again. So each element in the vector can be chosen to be in the random sample more than once\n\nsample(vec,size=5,replace = TRUE)\n\n#&gt; [1] 10  5  3  9  9\n\n\nsample() can be applied not only to numerical vectors, but other types of vectors such as character or logical.\nFor example, consider the character vector with the following names from our class. No one is volunteering so I randomly choose a set of three people in the class\n\nstudents &lt;- c('Leslie', 'Ron', 'Andy', 'April', 'Tom', 'Ben', 'Jerry')\n\n\nsample(students, 3)\n\n#&gt; [1] \"Leslie\" \"Tom\"    \"Andy\"\n\n\nIn this scenario sampling with replacement (i.e replace = TRUE) would not be appropriate\nOne important application for sampling is splitting up our dataset into two sets: one for training and one for testing an algorithm.\nWe will consider the penguins dataset from the palmerpenguins library\n\ndat &lt;- palmerpenguins::penguins\n\n\nnrow(dat)\n\n#&gt; [1] 344\n\n\nIn order to sample random rows from a dataset you first have to create a vector from 1:nrow(data), these will be the indices we will sample from\n\nrow_indices &lt;- 1:nrow(dat)\n\nOur training set will consist of 80% of the dataset and testing set will consist of the remaining 20%\n\ntrain_sampled_rows &lt;- sample(row_indices, size= 0.8*nrow(dat) )\n\nInterpretation: randomly select 80% (approx. 275) rows from 1:344\n\ntrain_sampled_rows[1:10]\n\n#&gt;  [1] 328  26   7 137 254 211  78  81  43 332\n\n\n\ntraining_set &lt;- dat[train_sampled_rows,]\ndim(training_set)\n\n#&gt; [1] 275   8\n\n\nlastly, we use the remaining rows for our testing set\n\ntest_set &lt;- dat[-train_sampled_rows, ]\ndim(test_set)\n\n#&gt; [1] 69  8"
  },
  {
    "objectID": "notes/08-sampling_simulation.html#sampling",
    "href": "notes/08-sampling_simulation.html#sampling",
    "title": "Sampling and Simulation",
    "section": "",
    "text": "We can obtain a random sample from existing elements in a dataset, vector, or list using the sample() function in R\nThe basic syntax for the sample() function is as follows:\nsample(x,size,replace = FALSE, prob = NULL)\n\nx: object we are sampling from, most often a vector or dataset\nsize: size of the sample\nreplace: should sampling be with replacement? (FALSE by default)\nprob: A vector of probability weights for obtaining the elements of the vector being sampled (NULL by default, every element is equally likely to be drawn)\n\nFor more details on the sample function run ?sample in the console\nIn order to replicate the following examples and get similar results, we can use set.seed()\n\nset.seed(123)\n\nSuppose we have a vector with 10 elements in it\n\nvec &lt;- 1:10\n\nTo generate a random sample of five elements from our vector vec, we run the following command\n\nsample(vec,size = 5)\n\n#&gt; [1]  3 10  2  8  6\n\n\nRecall, the default setting for sampling is without replacement. This means once we draw the number (or element) from our vector we can not draw it again, similar to the lottery method.\nIt is also important to note that each time we run the command sample() it may generate a different set of elements each time\n\nsample(vec,size = 5)\n\n#&gt; [1] 5 4 6 8 1\n\n\nWe can also sample random elements from our vector using the argument replace=TRUE so that we are sampling with replacement. That is, each time we draw an element we put it back into our vector and there may be a chance we draw it again. So each element in the vector can be chosen to be in the random sample more than once\n\nsample(vec,size=5,replace = TRUE)\n\n#&gt; [1] 10  5  3  9  9\n\n\nsample() can be applied not only to numerical vectors, but other types of vectors such as character or logical.\nFor example, consider the character vector with the following names from our class. No one is volunteering so I randomly choose a set of three people in the class\n\nstudents &lt;- c('Leslie', 'Ron', 'Andy', 'April', 'Tom', 'Ben', 'Jerry')\n\n\nsample(students, 3)\n\n#&gt; [1] \"Leslie\" \"Tom\"    \"Andy\"\n\n\nIn this scenario sampling with replacement (i.e replace = TRUE) would not be appropriate\nOne important application for sampling is splitting up our dataset into two sets: one for training and one for testing an algorithm.\nWe will consider the penguins dataset from the palmerpenguins library\n\ndat &lt;- palmerpenguins::penguins\n\n\nnrow(dat)\n\n#&gt; [1] 344\n\n\nIn order to sample random rows from a dataset you first have to create a vector from 1:nrow(data), these will be the indices we will sample from\n\nrow_indices &lt;- 1:nrow(dat)\n\nOur training set will consist of 80% of the dataset and testing set will consist of the remaining 20%\n\ntrain_sampled_rows &lt;- sample(row_indices, size= 0.8*nrow(dat) )\n\nInterpretation: randomly select 80% (approx. 275) rows from 1:344\n\ntrain_sampled_rows[1:10]\n\n#&gt;  [1] 328  26   7 137 254 211  78  81  43 332\n\n\n\ntraining_set &lt;- dat[train_sampled_rows,]\ndim(training_set)\n\n#&gt; [1] 275   8\n\n\nlastly, we use the remaining rows for our testing set\n\ntest_set &lt;- dat[-train_sampled_rows, ]\ndim(test_set)\n\n#&gt; [1] 69  8"
  },
  {
    "objectID": "notes/08-sampling_simulation.html#simulation",
    "href": "notes/08-sampling_simulation.html#simulation",
    "title": "Sampling and Simulation",
    "section": "Simulation",
    "text": "Simulation\nLet start with a simple example by simulating a fair dice roll\n\nsample(1:6,size=1)\n\n#&gt; [1] 2\n\n\nsince we are only drawing once, it doesn’t matter if you sample with or without replacement\nNow, consider rolling two fair, six-sided dice and computing their sum. One approach to compute this would be\n\ndice1 &lt;- sample(1:6, size=1)\ndice2 &lt;- sample(1:6, size=1)\ndice1 + dice2\n\n#&gt; [1] 9\n\n\nAn easier way:\n\nroll_two_dice &lt;- sample(1:6,size = 2,replace = TRUE)\n\n\nsum(roll_two_dice)\n\n#&gt; [1] 10\n\n\nWe are drawing two numbers from the range 1 through 6 with replacement. We sample with replacement because the roll of each dice is independent and it’s possible to roll (or draw) the same number twice. If we were to use replace=FALSE once we draw, say 1, we could not draw 1 again for the second dice.\nFrom the above examples, it is straightforward to carry out a simple random experiment. What if we wanted to repeat these experiment multiple times? For example, we wanted to repeat the experiment of drawing two dice and calculating their sum 100 or even 1000 times. For such scenarios we can utilize the replicate() function in R.\nreplicate() implements common tasks involving for loops without explicitly having to use looping syntax.\nThe basic syntax for the replicate() function is as follows:\nreplicate(n,expr, simplify = 'array')\n\nn: number of replications. How many times do you want to replicate the experiment\nexpr: the expression to evaluate repeatedly\nsimplify: how the output should be returned. The default is an array. If simplify=FALSE, the output will be a list of n elements\n\nLet’s start by simulating sampling 3 different numbers from ranges 1 to 20 at random without replacement, 10 times.\nBreaking it down, first we create our expression (or experiment)\n\nset.seed(10)\nsample(1:20, size=3, replace=FALSE)\n\n#&gt; [1] 11  9 10\n\n\nthen replicate this experiment 10 times\n\nreplicate(n = 10, \n          expr = sample(1:20, size=3, replace=FALSE))\n\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#&gt; [1,]   16    7   15   10    8    6   18    1    4    11\n#&gt; [2,]   12   19   10    2   14    7   13    7   18    15\n#&gt; [3,]    8   15    7   13    7   18    5   18   19     1\n\n\nThe default output of replicate() will be an array with n columns, the rows will depend on the length of the output from the experiment. For example, we are sampling 3 numbers from 1-20 so we will have three rows, each column will then correspond to a replicate of the experiment. So in the first replicate we drew the numbers (16,12,8). In the second replicate we drew the numbers (7,19,15), and similar interpretations hold for the remaining columns (replicates)\nGoing back to our example of calculating the sum of rolling two fair dice. We will replicate this experiment n times\nWe will use replicate as follows: 1. Write a function that performs the experiment once 2. Replicate the experiment using replicate() many times\nCreating a function\nWe can create a function that will calculate the sum of rolling two fair dice as follows\n\nsum_dice_roll &lt;- function(){\n  dice_roll &lt;- sample(1:6, size=2,replace = TRUE)\n  return(sum(dice_roll))\n}\n\nThe sum of rolling two fair dice once was\n\nsum_dice_roll()\n\n#&gt; [1] 6\n\n\nReplicate experiment multiple times\nLet’s start by calculating the sum of rolling two fair dice 20 times\n\nreplicate(n = 20,\n          expr = sum_dice_roll() )\n\n#&gt;  [1]  5 11  2  6  3  7  8  9  9  4  4  4  8 10  8  7  5  8  8  4\n\n\nreplicating our experiment 100 times\n\nrep100 &lt;- replicate(n = 100,\n                    expr = sum_dice_roll())\n\nIf we were to repeat this experiment many times, what is the sum that will most likely occur in the long run ?\nLooking at our experiment replicated 100 times we obtain the following relative frequency of every outcome. Note the possible outcomes are (2,3,4,…12)\n\nprop.table( table(rep100) ) \n\n#&gt; rep100\n#&gt;    2    3    4    5    6    7    8    9   10   11   12 \n#&gt; 0.01 0.04 0.06 0.07 0.14 0.21 0.09 0.12 0.14 0.08 0.04\n\n\n\nbarplot( prop.table(table(rep100)),\n         xlab = 'Sum', ylab = 'Relative Frequency')\n\n\n\n\nReplicating our experiments many more times, say 10,000 times, we will obtain more stable results which comes from the idea of “long-run” behavior. According to the law of large numbers, if we repeat an experiment independently a large number of times and average the result, we should obtain a value which is close to the actual expected value.\n\nrep10000 &lt;- replicate(n = 10000,\n                    expr = sum_dice_roll())\n\n\nprop.table( table(rep10000) ) \n\n#&gt; rep10000\n#&gt;      2      3      4      5      6      7      8      9     10     11     12 \n#&gt; 0.0285 0.0566 0.0810 0.1155 0.1418 0.1645 0.1372 0.1125 0.0805 0.0560 0.0259\n\n\n\nbarplot( prop.table(table(rep10000)),\n         xlab = 'Sum', ylab = 'Relative Frequency')\n\n\n\n\nDepending on the context of the problem 10,000 may not be considered large, but for the given dice roll experiment 10,000 is enough. The more replicates you perform the longer time it will take to run on your computer."
  },
  {
    "objectID": "notes/05-MOSAIC_basics.html",
    "href": "notes/05-MOSAIC_basics.html",
    "title": "MOSAIC Library",
    "section": "",
    "text": "The goal of the mosaic package is to make effective computation accessible to university-level students at an introductory level\nThe following packages from the mosaic suite will be used throughout the labs.\n\n{mosaic}\n\n{mosaicCore}\n{mosaicData}\n\n\nYou can the above packages by running the following command in the Console\n\ninstall.packages('mosaic')\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are installing the package for the first time, please be aware that it might take some time since there are numerous dependencies that need to be installed beforehand (this is done automatically)\n\n\nYou can see the remaining packages part of the mosaic suite on the Project MOSAIC Homepage, but they will not be used for this course\n\n\n\nIf you have successfully installed mosaic package you should be able to run the command library(mosaic) in the console without any errors. However, you will see the following messages\n\n1library(mosaic)\n#&gt; Registered S3 method overwritten by 'mosaic':\n#&gt;   method                           from   \n#&gt;   fortify.SpatialPolygonsDataFrame ggplot2\n#&gt; \n#&gt; The 'mosaic' package masks several functions from core packages in order to add \n2#&gt; additional features.  The original behavior of these functions should not be affected by this.\n#&gt; \n#&gt; Attaching package: 'mosaic'\n#&gt; The following objects are masked from 'package:dplyr':\n#&gt;     count, do, tally\n#&gt; The following object is masked from 'package:Matrix':\n#&gt;     mean\n#&gt; The following object is masked from 'package:ggplot2':\n#&gt;     stat\n#&gt; The following objects are masked from 'package:stats':\n#&gt;     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n#&gt;     quantile, sd, t.test, var\n#&gt;     \n#&gt; The following objects are masked from 'package:base':\n#&gt;     max, mean, min, prod, range, sample, sum\n\n\n1\n\nMake sure to load mosiac package first, you only have to do this once at the beginning of your script. Proceeding, this tutorial will assume you have loaded the mosaic package\n\n2\n\nThe output above are simply messages when initially starting up the package warning you about masking of functions with the same name as different packages\n\n\n\n\nFor example, the function binom.test() is used in two different packages mosaic and stats. If two packages use the same function name, then the package loaded last will hide the function from earlier packages. This is called masking\nWhile in general, the order of packages being loaded does not matter, if you are using multiple packages which have functions with the exact same name it is better to explicitly call the function name using package_name::function\n\nmosaic::binom.test()\nstats::binom.test()"
  },
  {
    "objectID": "notes/05-MOSAIC_basics.html#examples",
    "href": "notes/05-MOSAIC_basics.html#examples",
    "title": "MOSAIC Library",
    "section": "Examples",
    "text": "Examples\n\nNumerical Summaries\nConsider the following vector containing various types of animals (mostly pets)\n\nanimals &lt;- c(\"fish\", \"cat\",  \"fish\", \"cat\"  ,\"bird\" ,\"fish\" ,\"bird\" ,\n             \"bird\" ,\"dog\"  ,\"cat\"  ,\"dog\"  ,\"dog\"  ,\"cat\", \"dog\",  \n             \"dog\",  \"cat\",  \"bird\", \"cat\" , \"bird\", \"fish\")\n\nloading the mosaic package and using the tally() function\n\ntally(animals)\n\n#&gt; X\n#&gt; bird  cat  dog fish \n#&gt;    5    6    5    4\n\n\nwe can obtain the counts for each animal. By default tally() gives us counts, but we can also display the proportions using the format argument\n\ntally(animals, format = 'proportion')\n\n#&gt; X\n#&gt; bird  cat  dog fish \n#&gt; 0.25 0.30 0.25 0.20\n\n\nwhich is more readable than using base R\n\nprop.table(table(animals))\n\n#&gt; animals\n#&gt; bird  cat  dog fish \n#&gt; 0.25 0.30 0.25 0.20\n\n\nReading the documentation ?mosaicCore::tally, format can take one of the following types\n\nc(\"count\", \"proportion\", \"percent\", \"data.frame\", \"sparse\", \"default\")\n\n\ntally(animals, format = 'percent')\n\n#&gt; X\n#&gt; bird  cat  dog fish \n#&gt;   25   30   25   20\n\n\n\n\nSimulation\nFor simulation the do() function from mosaic provides a natural syntax for repetition tuned to assist with replication and resampling methods. Consider the following example\n\n1do_hello &lt;- do(3) * 'hello'\ndo_hello\n\n\n1\n\nInterpretation: write ‘hello’ three times\n\n\n\n\n#&gt;   hello\n#&gt; 1 hello\n#&gt; 2 hello\n#&gt; 3 hello\n\n\nWe are going to “do” the word three times do(3) using the operator * not to confuse with multiplication\n\nstr(do_hello)\n\n#&gt; Classes 'do.data.frame' and 'data.frame':    3 obs. of  1 variable:\n#&gt;  $ hello: chr  \"hello\" \"hello\" \"hello\"\n#&gt;  - attr(*, \"lazy\")= language ~\"hello\"\n#&gt;   ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n#&gt;  - attr(*, \"culler\")=function (object, ...)\n\n\ndo returns class do.data.frame and data.frame where each row is the output after performing the task after the * operator one time\n\ndo_hello$hello\n\n#&gt; [1] \"hello\" \"hello\" \"hello\"\n\n\nIf you want to extract the values from the generated data.frame, we can use the $ operator\nIn the next example, we are going to write the numbers 1:4 five times\n\ndo(5) * 1:4\n\n#&gt;   V1 V2 V3 V4\n#&gt; 1  1  2  3  4\n#&gt; 2  1  2  3  4\n#&gt; 3  1  2  3  4\n#&gt; 4  1  2  3  4\n#&gt; 5  1  2  3  4\n\n\nagain the output will be a data.frame where each row is the numbers 1:4, you can think of each row as a replication of the desired task. In some way, it is similar to the base R approach of using replicate\n\nreplicate(n=5, 1:4)\n\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1    1    1    1    1\n#&gt; [2,]    2    2    2    2    2\n#&gt; [3,]    3    3    3    3    3\n#&gt; [4,]    4    4    4    4    4\n\n\nAnother function from mosaic to consider is rflip(). The function rflip simulates coin tosses, with heads as success and tails as failures.\n\n1coin_toss &lt;- rflip(n=5)\ncoin_toss\n\n\n1\n\nFlip a fair coin five times\n\n\n\n\n#&gt; \n#&gt; Flipping 5 coins [ Prob(Heads) = 0.5 ] ...\n#&gt; \n#&gt; T H T T T\n#&gt; \n#&gt; Number of Heads: 1 [Proportion Heads: 0.2]\n\n\n\nstr(coin_toss)\n\n#&gt;  'cointoss' int 1\n#&gt;  - attr(*, \"n\")= num 5\n#&gt;  - attr(*, \"prob\")= num 0.5\n#&gt;  - attr(*, \"sequence\")= chr [1:5] \"T\" \"H\" \"T\" \"T\" ...\n#&gt;  - attr(*, \"verbose\")= logi TRUE\n\n\nThe output of rflip is a cointoss object, which displays the number of heads alongside the corresponding proportion of heads. We can change the probability of obtaining heads making the coin unfair. For example,\n\n1rflip(n = 10, prob = 0.3)\n\n\n1\n\nFlip 10 coins, each with probability of 0.3 of landing on heads\n\n\n\n\n#&gt; \n#&gt; Flipping 10 coins [ Prob(Heads) = 0.3 ] ...\n#&gt; \n#&gt; T T T T T T T H T T\n#&gt; \n#&gt; Number of Heads: 1 [Proportion Heads: 0.1]\n\n\nwe can summarize the above output and return a data.frame which we can extract information from\n\ncoin_toss_data &lt;- rflip(n = 10, prob = 0.3, summarize = TRUE)\n\n\n\n#&gt;    n heads tails prob\n#&gt; 1 10     2     8  0.3\n\n\n\nstr(coin_toss_data)\n\n#&gt; 'data.frame':    1 obs. of  4 variables:\n#&gt;  $ n    : num 10\n#&gt;  $ heads: int 2\n#&gt;  $ tails: num 8\n#&gt;  $ prob : num 0.3\n\n\n\n\n\nCombining do() and rflip()\nConsider the following example,\n\n1result &lt;- do(2) * rflip(n=5)\nresult\n\n\n1\n\nRandomly flip a fair coin 5 times and replicate this procedure two times\n\n\n\n\n#&gt;   n heads tails prop\n#&gt; 1 5     1     4  0.2\n#&gt; 2 5     1     4  0.2\n\n\nRecall the output after performing a do() statement will be a data.frame where each row is a replication of the procedure following the * operator. The first row shows the number of tosses, how many heads/tails where obtained and the corresponding proportion of heads after n tosses\n\n\n#&gt; Classes 'do.data.frame' and 'data.frame':    2 obs. of  4 variables:\n#&gt;  $ n    : num  5 5\n#&gt;  $ heads: num  1 1\n#&gt;  $ tails: num  4 4\n#&gt;  $ prop : num  0.2 0.2\n#&gt;  - attr(*, \"lazy\")= language ~rflip(n = 5)\n#&gt;   ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n#&gt;  - attr(*, \"culler\")=function (object, ...)"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  }
]